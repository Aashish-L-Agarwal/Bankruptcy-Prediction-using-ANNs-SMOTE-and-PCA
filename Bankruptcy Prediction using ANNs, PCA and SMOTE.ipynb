{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f1471a",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 35px; color: lightblue; text-align:center; font-weight: bolder;\"> Bankruptcy Prediction: Artificial Neural Networks, Borderline SMOTE and Principal Component Analysis </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe7b29",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff549d",
   "metadata": {},
   "source": [
    "<!-- Add font awesome icons -->\n",
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\" />\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/js/all.min.js\"></script>\n",
    "\n",
    "<!-- Contents -->\n",
    "<a id=\"index\" style=\"font-size: 28px; color: lightblue; font-weight:bolder; text-decoration: none;\">Contents</a>\n",
    "<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">\n",
    "\n",
    "<a href=\"#1\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\">1. Importing data and libraries</a>\n",
    "\n",
    "<a href=\"#2\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">2. Exploratory Data Analysis (EDA)</a>\n",
    "<ul style=\"color: #033a91\">\n",
    "    <li><a href=\"#2.1\" style=\"color: #033a91; text-decoration: none;\"> 2.1 Missing and Duplicate Data</a></li>\n",
    "  <li><a href=\"#2.2\" style=\"color: #033a91; text-decoration: none;\">2.2 Class Distribution</a></li>\n",
    "  <li><a href=\"#2.3\" style=\"color: #033a91; text-decoration: none;\">2.3 Outliers Treatment</a></li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"#3\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">3. Training and Scaling Data</a>\n",
    "\n",
    "<a href=\"#4\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">4. SMOTE Oversampling</a>\n",
    "\n",
    "<a href=\"#5\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">5. Baseline Model: Logistic Regression</a>\n",
    "\n",
    "<a href=\"#6\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">6. Model 1: 5-layer-ANN-SMOTE</a>\n",
    "<ul style=\"color: #033a91\">\n",
    "    <li><a href=\"#6.1\" style=\"color: #033a91; text-decoration: none;\">6.1 Tuning the hyperparameters</a></li>\n",
    "  <li><a href=\"#6.2\" style=\"color: #033a91; text-decoration: none;\">6.2 Building the model</a></li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"#7\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">7. Model 2: 6-layer-ANN-SMOTE</a>\n",
    "<ul style=\"color: #033a91\">\n",
    "    <li><a href=\"#7.1\" style=\"color: #033a91; text-decoration: none;\">7.1 Tuning the hyperparameters</a></li>\n",
    "  <li><a href=\"#7.2\" style=\"color: #033a91; text-decoration: none;\">7.2 Building the model</a></li>\n",
    "</ul>\n",
    "\n",
    "<a href=\"#8\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\">8. Model 3: 5-layer-ANN-PCA-BSM</a>\n",
    "<ul style=\"color: #033a91\">\n",
    "    <li><a href=\"#8.1\" style=\"color: #033a91; text-decoration: none;\">8.1 PCA</a></li>\n",
    "  <li><a href=\"#8.2\" style=\"color: #033a91; text-decoration: none;\">8.2 BSM Oversampling</a></li>\n",
    "      <li><a href=\"#8.3\" style=\"color: #033a91; text-decoration: none;\">8.3 Tuning the hyperparameters</a></li>\n",
    "    <li><a href=\"#8.4\" style=\"color: #033a91; text-decoration: none;\">8.4 Building the model</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be033f51",
   "metadata": {},
   "source": [
    "<br id=\"1\">\n",
    "<a href=\"#index\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\"> 1. Importing data and libraries </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44426f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from bayesian-optimization) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from bayesian-optimization) (1.21.5)\n",
      "Requirement already satisfied: colorama>=0.4.6 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from bayesian-optimization) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc46afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in /Users/aashish/opt/anaconda3/lib/python3.9/site-packages (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3839f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:19:14.726102: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, balanced_accuracy_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd54bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Recall, Precision\n",
    "from numpy import float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = pd.read_csv(\"data.csv\", index_col=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdb4f8",
   "metadata": {},
   "source": [
    "<br id=\"2\">\n",
    "<a href=\"#index\" style=\"color:#033a91;  font-size: 22px; text-decoration: none; font-weight:bold;\"> 2. Exploratory Data Analysis (EDA) </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9d245",
   "metadata": {},
   "source": [
    "<br id=\"2.1\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">  2.1 Missing and Duplicate Data<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67989e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NaNs check\n",
    "\n",
    "[print(col) for col in company if company[col].isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cece43e",
   "metadata": {},
   "source": [
    "Despite the fact that we already know that there are no missing values (here it's pretty easy considering that we have just 96 features) it is important to computationally check that this is true, to avoid errors and time wasted in the following steps of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a436af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Duplicate values\n",
    "\n",
    "company.drop_duplicates(inplace=True) #no change in no. of datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96f02d",
   "metadata": {},
   "source": [
    "<br id=\"2.2\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\"> 2.2 Class Distribution</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9942e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6599\n",
      "1     220\n",
      "Name: Bankrupt?, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_counts = company['Bankrupt?'].value_counts()\n",
    "print(target_counts)\n",
    "print(\"\")\n",
    "\n",
    "#HEAVILY SKEWED as only 3.23% of the companies in this dataset are financially unstable / bankrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b65ef",
   "metadata": {},
   "source": [
    "<br id=\"2.3\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\"> 2.3 Outlier Treatment</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ab69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower quartile (25%) is: 0.0 and Upper Quartile (75%) is: 0.0\n",
      "iqr: 0.0\n",
      "Cut Off is: 0.0\n",
      "Bankrupt? Lower: 0.0\n",
      "Bankrupt? Upper: 0.0\n",
      "For Bankrupt?,total outlier cases: 220\n",
      "\n",
      "Lower quartile (25%) is: 0.476527080388047 and Upper Quartile (75%) is: 0.535562813825379\n",
      "iqr: 0.05903573343733198\n",
      "Cut Off is: 0.08855360015599797\n",
      " ROA(C) before interest and depreciation before interest Lower: 0.38797348023204903\n",
      " ROA(C) before interest and depreciation before interest Upper: 0.624116413981377\n",
      "For  ROA(C) before interest and depreciation before interest,total outlier cases: 391\n",
      "\n",
      "Lower quartile (25%) is: 0.53554295682512 and Upper Quartile (75%) is: 0.58915721761884\n",
      "iqr: 0.05361426079372\n",
      "Cut Off is: 0.08042139119058\n",
      " ROA(A) before interest and % after tax Lower: 0.45512156563454\n",
      " ROA(A) before interest and % after tax Upper: 0.66957860880942\n",
      "For  ROA(A) before interest and % after tax,total outlier cases: 561\n",
      "\n",
      "Lower quartile (25%) is: 0.527276620804112 and Upper Quartile (75%) is: 0.584105144815033\n",
      "iqr: 0.05682852401092098\n",
      "Cut Off is: 0.08524278601638147\n",
      " ROA(B) before interest and depreciation after tax Lower: 0.4420338347877305\n",
      " ROA(B) before interest and depreciation after tax Upper: 0.6693479308314144\n",
      "For  ROA(B) before interest and depreciation after tax,total outlier cases: 432\n",
      "\n",
      "Lower quartile (25%) is: 0.6004446590466855 and Upper Quartile (75%) is: 0.613914152697502\n",
      "iqr: 0.01346949365081651\n",
      "Cut Off is: 0.020204240476224766\n",
      " Operating Gross Margin Lower: 0.5802404185704608\n",
      " Operating Gross Margin Upper: 0.6341183931737269\n",
      "For  Operating Gross Margin,total outlier cases: 320\n",
      "\n",
      "Lower quartile (25%) is: 0.600433848859165 and Upper Quartile (75%) is: 0.6138420847806975\n",
      "iqr: 0.013408235921532508\n",
      "Cut Off is: 0.02011235388229876\n",
      " Realized Sales Gross Margin Lower: 0.5803214949768662\n",
      " Realized Sales Gross Margin Upper: 0.6339544386629963\n",
      "For  Realized Sales Gross Margin,total outlier cases: 318\n",
      "\n",
      "Lower quartile (25%) is: 0.998969203197885 and Upper Quartile (75%) is: 0.999094514164357\n",
      "iqr: 0.00012531096647205864\n",
      "Cut Off is: 0.00018796644970808796\n",
      " Operating Profit Rate Lower: 0.9987812367481769\n",
      " Operating Profit Rate Upper: 0.9992824806140651\n",
      "For  Operating Profit Rate,total outlier cases: 716\n",
      "\n",
      "Lower quartile (25%) is: 0.797385863236893 and Upper Quartile (75%) is: 0.797578848185589\n",
      "iqr: 0.0001929849486960178\n",
      "Cut Off is: 0.0002894774230440267\n",
      " Pre-tax net Interest Rate Lower: 0.7970963858138489\n",
      " Pre-tax net Interest Rate Upper: 0.797868325608633\n",
      "For  Pre-tax net Interest Rate,total outlier cases: 773\n",
      "\n",
      "Lower quartile (25%) is: 0.809311597146491 and Upper Quartile (75%) is: 0.809469266134837\n",
      "iqr: 0.00015766898834601584\n",
      "Cut Off is: 0.00023650348251902376\n",
      " After-tax net Interest Rate Lower: 0.809075093663972\n",
      " After-tax net Interest Rate Upper: 0.8097057696173561\n",
      "For  After-tax net Interest Rate,total outlier cases: 867\n",
      "\n",
      "Lower quartile (25%) is: 0.30346627659685 and Upper Quartile (75%) is: 0.303585192461218\n",
      "iqr: 0.00011891586436801882\n",
      "Cut Off is: 0.00017837379655202823\n",
      " Non-industry income and expenditure/revenue Lower: 0.30328790280029794\n",
      " Non-industry income and expenditure/revenue Upper: 0.30376356625777\n",
      "For  Non-industry income and expenditure/revenue,total outlier cases: 1094\n",
      "\n",
      "Lower quartile (25%) is: 0.7815668165898519 and Upper Quartile (75%) is: 0.7817353784192015\n",
      "iqr: 0.000168561829349545\n",
      "Cut Off is: 0.0002528427440243175\n",
      " Continuous interest rate (after tax) Lower: 0.7813139738458277\n",
      " Continuous interest rate (after tax) Upper: 0.7819882211632259\n",
      "For  Continuous interest rate (after tax),total outlier cases: 806\n",
      "\n",
      "Lower quartile (25%) is: 0.0001566874492428 and Upper Quartile (75%) is: 4145000000.0\n",
      "iqr: 4144999999.999843\n",
      "Cut Off is: 6217499999.999764\n",
      " Operating Expense Rate Lower: -6217499999.999608\n",
      " Operating Expense Rate Upper: 10362499999.999763\n",
      "For  Operating Expense Rate,total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.000128187953762 and Upper Quartile (75%) is: 3450000000.0\n",
      "iqr: 3449999999.9998717\n",
      "Cut Off is: 5174999999.999807\n",
      " Research and development expense rate Lower: -5174999999.99968\n",
      " Research and development expense rate Upper: 8624999999.999807\n",
      "For  Research and development expense rate,total outlier cases: 182\n",
      "\n",
      "Lower quartile (25%) is: 0.4615577531181065 and Upper Quartile (75%) is: 0.471003917029432\n",
      "iqr: 0.009446163911325522\n",
      "Cut Off is: 0.014169245866988284\n",
      " Cash flow rate Lower: 0.44738850725111823\n",
      " Cash flow rate Upper: 0.4851731628964203\n",
      "For  Cash flow rate,total outlier cases: 576\n",
      "\n",
      "Lower quartile (25%) is: 0.0002030203020302 and Upper Quartile (75%) is: 0.0005325532553255\n",
      "iqr: 0.00032953295329529996\n",
      "Cut Off is: 0.00049429942994295\n",
      " Interest-bearing debt interest rate Lower: -0.00029127912791275\n",
      " Interest-bearing debt interest rate Upper: 0.00102685268526845\n",
      "For  Interest-bearing debt interest rate,total outlier cases: 396\n",
      "\n",
      "Lower quartile (25%) is: 0.0 and Upper Quartile (75%) is: 0.205840672132807\n",
      "iqr: 0.205840672132807\n",
      "Cut Off is: 0.3087610081992105\n",
      " Tax rate (A) Lower: -0.3087610081992105\n",
      " Tax rate (A) Upper: 0.5146016803320175\n",
      "For  Tax rate (A),total outlier cases: 120\n",
      "\n",
      "Lower quartile (25%) is: 0.173612574269942 and Upper Quartile (75%) is: 0.199570182461759\n",
      "iqr: 0.025957608191817\n",
      "Cut Off is: 0.0389364122877255\n",
      " Net Value Per Share (B) Lower: 0.1346761619822165\n",
      " Net Value Per Share (B) Upper: 0.2385065947494845\n",
      "For  Net Value Per Share (B),total outlier cases: 457\n",
      "\n",
      "Lower quartile (25%) is: 0.173612574269942 and Upper Quartile (75%) is: 0.199570182461759\n",
      "iqr: 0.025957608191817\n",
      "Cut Off is: 0.0389364122877255\n",
      " Net Value Per Share (A) Lower: 0.1346761619822165\n",
      " Net Value Per Share (A) Upper: 0.2385065947494845\n",
      "For  Net Value Per Share (A),total outlier cases: 464\n",
      "\n",
      "Lower quartile (25%) is: 0.1736757827314485 and Upper Quartile (75%) is: 0.199612321436096\n",
      "iqr: 0.025936538704647488\n",
      "Cut Off is: 0.03890480805697123\n",
      " Net Value Per Share (C) Lower: 0.1347709746744773\n",
      " Net Value Per Share (C) Upper: 0.23851712949306725\n",
      "For  Net Value Per Share (C),total outlier cases: 465\n",
      "\n",
      "Lower quartile (25%) is: 0.214711165736976 and Upper Quartile (75%) is: 0.2388200813085\n",
      "iqr: 0.024108915571524003\n",
      "Cut Off is: 0.036163373357286005\n",
      " Persistent EPS in the Last Four Seasons Lower: 0.17854779237968998\n",
      " Persistent EPS in the Last Four Seasons Upper: 0.274983454665786\n",
      "For  Persistent EPS in the Last Four Seasons,total outlier cases: 508\n",
      "\n",
      "Lower quartile (25%) is: 0.317747754120393 and Upper Quartile (75%) is: 0.3286234703260945\n",
      "iqr: 0.010875716205701491\n",
      "Cut Off is: 0.016313574308552237\n",
      " Cash Flow Per Share Lower: 0.3014341798118408\n",
      " Cash Flow Per Share Upper: 0.34493704463464675\n",
      "For  Cash Flow Per Share,total outlier cases: 532\n",
      "\n",
      "Lower quartile (25%) is: 0.01563138073415305 and Upper Quartile (75%) is: 0.0463572152396509\n",
      "iqr: 0.030725834505497845\n",
      "Cut Off is: 0.04608875175824677\n",
      " Revenue Per Share (Yuan ¥) Lower: -0.030457371024093717\n",
      " Revenue Per Share (Yuan ¥) Upper: 0.09244596699789767\n",
      "For  Revenue Per Share (Yuan ¥),total outlier cases: 478\n",
      "\n",
      "Lower quartile (25%) is: 0.0960833808321798 and Upper Quartile (75%) is: 0.1161550362348345\n",
      "iqr: 0.02007165540265471\n",
      "Cut Off is: 0.030107483103982063\n",
      " Operating Profit Per Share (Yuan ¥) Lower: 0.06597589772819773\n",
      " Operating Profit Per Share (Yuan ¥) Upper: 0.14626251933881657\n",
      "For  Operating Profit Per Share (Yuan ¥),total outlier cases: 442\n",
      "\n",
      "Lower quartile (25%) is: 0.170369812457634 and Upper Quartile (75%) is: 0.193492505837162\n",
      "iqr: 0.023122693379527992\n",
      "Cut Off is: 0.03468404006929199\n",
      " Per Share Net profit before tax (Yuan ¥) Lower: 0.135685772388342\n",
      " Per Share Net profit before tax (Yuan ¥) Upper: 0.22817654590645398\n",
      "For  Per Share Net profit before tax (Yuan ¥),total outlier cases: 511\n",
      "\n",
      "Lower quartile (25%) is: 0.022064532735505453 and Upper Quartile (75%) is: 0.022153148426612798\n",
      "iqr: 8.861569110734518e-05\n",
      "Cut Off is: 0.00013292353666101778\n",
      " Realized Sales Gross Profit Growth Rate Lower: 0.021931609198844435\n",
      " Realized Sales Gross Profit Growth Rate Upper: 0.022286071963273816\n",
      "For  Realized Sales Gross Profit Growth Rate,total outlier cases: 814\n",
      "\n",
      "Lower quartile (25%) is: 0.8479841081819834 and Upper Quartile (75%) is: 0.8481225403945605\n",
      "iqr: 0.0001384322125770332\n",
      "Cut Off is: 0.0002076483188655498\n",
      " Operating Profit Growth Rate Lower: 0.8477764598631179\n",
      " Operating Profit Growth Rate Upper: 0.848330188713426\n",
      "For  Operating Profit Growth Rate,total outlier cases: 1008\n",
      "\n",
      "Lower quartile (25%) is: 0.6892699337448115 and Upper Quartile (75%) is: 0.6896471679790515\n",
      "iqr: 0.00037723423423996483\n",
      "Cut Off is: 0.0005658513513599472\n",
      " After-tax Net Profit Growth Rate Lower: 0.6887040823934516\n",
      " After-tax Net Profit Growth Rate Upper: 0.6902130193304115\n",
      "For  After-tax Net Profit Growth Rate,total outlier cases: 1033\n",
      "\n",
      "Lower quartile (25%) is: 0.689270265563206 and Upper Quartile (75%) is: 0.6896470092832976\n",
      "iqr: 0.00037674372009155466\n",
      "Cut Off is: 0.000565115580137332\n",
      " Regular Net Profit Growth Rate Lower: 0.6887051499830688\n",
      " Regular Net Profit Growth Rate Upper: 0.690212124863435\n",
      "For  Regular Net Profit Growth Rate,total outlier cases: 1030\n",
      "\n",
      "Lower quartile (25%) is: 0.2175795122117655 and Upper Quartile (75%) is: 0.217621501194243\n",
      "iqr: 4.198898247750882e-05\n",
      "Cut Off is: 6.298347371626323e-05\n",
      " Continuous Net Profit Growth Rate Lower: 0.21751652873804922\n",
      " Continuous Net Profit Growth Rate Upper: 0.21768448466795925\n",
      "For  Continuous Net Profit Growth Rate,total outlier cases: 1042\n",
      "\n",
      "Lower quartile (25%) is: 4860000000.0 and Upper Quartile (75%) is: 7390000000.0\n",
      "iqr: 2530000000.0\n",
      "Cut Off is: 3795000000.0\n",
      " Total Asset Growth Rate Lower: 1065000000.0\n",
      " Total Asset Growth Rate Upper: 11185000000.0\n",
      "For  Total Asset Growth Rate,total outlier cases: 1381\n",
      "\n",
      "Lower quartile (25%) is: 0.0004409688868264 and Upper Quartile (75%) is: 0.000499362141038\n",
      "iqr: 5.839325421160002e-05\n",
      "Cut Off is: 8.758988131740003e-05\n",
      " Net Value Growth Rate Lower: 0.00035337900550899996\n",
      " Net Value Growth Rate Upper: 0.0005869520223554\n",
      "For  Net Value Growth Rate,total outlier cases: 792\n",
      "\n",
      "Lower quartile (25%) is: 0.263758926420651 and Upper Quartile (75%) is: 0.264388341065032\n",
      "iqr: 0.0006294146443809878\n",
      "Cut Off is: 0.0009441219665714817\n",
      " Total Asset Return Growth Rate Ratio Lower: 0.26281480445407956\n",
      " Total Asset Return Growth Rate Ratio Upper: 0.2653324630316035\n",
      "For  Total Asset Return Growth Rate Ratio,total outlier cases: 674\n",
      "\n",
      "Lower quartile (25%) is: 0.37474851905666695 and Upper Quartile (75%) is: 0.386731120301032\n",
      "iqr: 0.011982601244365065\n",
      "Cut Off is: 0.017973901866547598\n",
      " Cash Reinvestment % Lower: 0.3567746171901194\n",
      " Cash Reinvestment % Upper: 0.40470502216757964\n",
      "For  Cash Reinvestment %,total outlier cases: 617\n",
      "\n",
      "Lower quartile (25%) is: 0.00755504663011965 and Upper Quartile (75%) is: 0.0162695280201934\n",
      "iqr: 0.00871448139007375\n",
      "Cut Off is: 0.013071722085110626\n",
      " Current Ratio Lower: -0.005516675454990976\n",
      " Current Ratio Upper: 0.02934125010530403\n",
      "For  Current Ratio,total outlier cases: 589\n",
      "\n",
      "Lower quartile (25%) is: 0.004725903227376101 and Upper Quartile (75%) is: 0.01224910697241505\n",
      "iqr: 0.00752320374503895\n",
      "Cut Off is: 0.011284805617558425\n",
      " Quick Ratio Lower: -0.006558902390182324\n",
      " Quick Ratio Upper: 0.023533912589973477\n",
      "For  Quick Ratio,total outlier cases: 591\n",
      "\n",
      "Lower quartile (25%) is: 0.63061225188696 and Upper Quartile (75%) is: 0.631125258558102\n",
      "iqr: 0.0005130066711419579\n",
      "Cut Off is: 0.0007695100067129368\n",
      " Interest Expense Ratio Lower: 0.629842741880247\n",
      " Interest Expense Ratio Upper: 0.6318947685648149\n",
      "For  Interest Expense Ratio,total outlier cases: 1362\n",
      "\n",
      "Lower quartile (25%) is: 0.0030070491250148 and Upper Quartile (75%) is: 0.00927329266179695\n",
      "iqr: 0.0062662435367821494\n",
      "Cut Off is: 0.009399365305173223\n",
      " Total debt/Total net worth Lower: -0.006392316180158423\n",
      " Total debt/Total net worth Upper: 0.018672657966970173\n",
      "For  Total debt/Total net worth,total outlier cases: 407\n",
      "\n",
      "Lower quartile (25%) is: 0.0728905281615624 and Upper Quartile (75%) is: 0.148804305106267\n",
      "iqr: 0.0759137769447046\n",
      "Cut Off is: 0.1138706654170569\n",
      " Debt ratio % Lower: -0.0409801372554945\n",
      " Debt ratio % Upper: 0.2626749705233239\n",
      "For  Debt ratio %,total outlier cases: 30\n",
      "\n",
      "Lower quartile (25%) is: 0.8511956948937329 and Upper Quartile (75%) is: 0.927109471838438\n",
      "iqr: 0.07591377694470502\n",
      "Cut Off is: 0.11387066541705754\n",
      " Net worth/Assets Lower: 0.7373250294766753\n",
      " Net worth/Assets Upper: 1.0409801372554954\n",
      "For  Net worth/Assets,total outlier cases: 30\n",
      "\n",
      "Lower quartile (25%) is: 0.0052436836906082 and Upper Quartile (75%) is: 0.00684743246553585\n",
      "iqr: 0.0016037487749276502\n",
      "Cut Off is: 0.0024056231623914752\n",
      " Long-term fund suitability ratio (A) Lower: 0.0028380605282167246\n",
      " Long-term fund suitability ratio (A) Upper: 0.009253055627927324\n",
      "For  Long-term fund suitability ratio (A),total outlier cases: 810\n",
      "\n",
      "Lower quartile (25%) is: 0.3701678435547765 and Upper Quartile (75%) is: 0.3762707372009225\n",
      "iqr: 0.006102893646146024\n",
      "Cut Off is: 0.009154340469219036\n",
      " Borrowing dependency Lower: 0.36101350308555746\n",
      " Borrowing dependency Upper: 0.38542507767014156\n",
      "For  Borrowing dependency,total outlier cases: 321\n",
      "\n",
      "Lower quartile (25%) is: 0.0053658477137564 and Upper Quartile (75%) is: 0.00576435604952715\n",
      "iqr: 0.0003985083357707494\n",
      "Cut Off is: 0.0005977625036561241\n",
      " Contingent liabilities/Net worth Lower: 0.004768085210100276\n",
      " Contingent liabilities/Net worth Upper: 0.0063621185531832734\n",
      "For  Contingent liabilities/Net worth,total outlier cases: 942\n",
      "\n",
      "Lower quartile (25%) is: 0.0961046786197013 and Upper Quartile (75%) is: 0.115927337274252\n",
      "iqr: 0.0198226586545507\n",
      "Cut Off is: 0.029733987981826047\n",
      " Operating profit/Paid-in capital Lower: 0.06637069063787526\n",
      " Operating profit/Paid-in capital Upper: 0.14566132525607806\n",
      "For  Operating profit/Paid-in capital,total outlier cases: 446\n",
      "\n",
      "Lower quartile (25%) is: 0.169376366789835 and Upper Quartile (75%) is: 0.191606967800317\n",
      "iqr: 0.022230601010481993\n",
      "Cut Off is: 0.03334590151572299\n",
      " Net profit before tax/Paid-in capital Lower: 0.136030465274112\n",
      " Net profit before tax/Paid-in capital Upper: 0.22495286931603997\n",
      "For  Net profit before tax/Paid-in capital,total outlier cases: 476\n",
      "\n",
      "Lower quartile (25%) is: 0.3974026791778925 and Upper Quartile (75%) is: 0.404550770809581\n",
      "iqr: 0.007148091631688502\n",
      "Cut Off is: 0.010722137447532754\n",
      " Inventory and accounts receivable/Net value Lower: 0.38668054173035976\n",
      " Inventory and accounts receivable/Net value Upper: 0.41527290825711377\n",
      "For  Inventory and accounts receivable/Net value,total outlier cases: 421\n",
      "\n",
      "Lower quartile (25%) is: 0.0764617691154423 and Upper Quartile (75%) is: 0.176911544227886\n",
      "iqr: 0.1004497751124437\n",
      "Cut Off is: 0.15067466266866555\n",
      " Total Asset Turnover Lower: -0.07421289355322326\n",
      " Total Asset Turnover Upper: 0.32758620689655155\n",
      "For  Total Asset Turnover,total outlier cases: 351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lower quartile (25%) is: 0.0007101336065656 and Upper Quartile (75%) is: 0.0014547594168788\n",
      "iqr: 0.0007446258103132001\n",
      "Cut Off is: 0.0011169387154698002\n",
      " Accounts Receivable Turnover Lower: -0.0004068051089042002\n",
      " Accounts Receivable Turnover Upper: 0.0025716981323486003\n",
      "For  Accounts Receivable Turnover,total outlier cases: 659\n",
      "\n",
      "Lower quartile (25%) is: 0.0043865304397204 and Upper Quartile (75%) is: 0.00897287558119175\n",
      "iqr: 0.004586345141471349\n",
      "Cut Off is: 0.006879517712207023\n",
      " Average Collection Days Lower: -0.002492987272486623\n",
      " Average Collection Days Upper: 0.015852393293398773\n",
      "For  Average Collection Days,total outlier cases: 193\n",
      "\n",
      "Lower quartile (25%) is: 0.0001728255554827 and Upper Quartile (75%) is: 4620000000.0\n",
      "iqr: 4619999999.999827\n",
      "Cut Off is: 6929999999.999741\n",
      " Inventory Turnover Rate (times) Lower: -6929999999.999568\n",
      " Inventory Turnover Rate (times) Upper: 11549999999.99974\n",
      "For  Inventory Turnover Rate (times),total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.0002330013064716 and Upper Quartile (75%) is: 0.0036523711287173\n",
      "iqr: 0.0034193698222457\n",
      "Cut Off is: 0.00512905473336855\n",
      " Fixed Assets Turnover Frequency Lower: -0.00489605342689695\n",
      " Fixed Assets Turnover Frequency Upper: 0.00878142586208585\n",
      "For  Fixed Assets Turnover Frequency,total outlier cases: 1418\n",
      "\n",
      "Lower quartile (25%) is: 0.0217741935483871 and Upper Quartile (75%) is: 0.0429032258064516\n",
      "iqr: 0.021129032258064503\n",
      "Cut Off is: 0.03169354838709675\n",
      " Net Worth Turnover Rate (times) Lower: -0.009919354838709652\n",
      " Net Worth Turnover Rate (times) Upper: 0.07459677419354835\n",
      "For  Net Worth Turnover Rate (times),total outlier cases: 513\n",
      "\n",
      "Lower quartile (25%) is: 0.010432854016421151 and Upper Quartile (75%) is: 0.0358547655068079\n",
      "iqr: 0.025421911490386747\n",
      "Cut Off is: 0.03813286723558012\n",
      " Revenue per person Lower: -0.027700013219158968\n",
      " Revenue per person Upper: 0.07398763274238801\n",
      "For  Revenue per person,total outlier cases: 729\n",
      "\n",
      "Lower quartile (25%) is: 0.392437981954275 and Upper Quartile (75%) is: 0.40185093055335697\n",
      "iqr: 0.009412948599081983\n",
      "Cut Off is: 0.014119422898622974\n",
      " Operating profit per person Lower: 0.37831855905565204\n",
      " Operating profit per person Upper: 0.41597035345197997\n",
      "For  Operating profit per person,total outlier cases: 876\n",
      "\n",
      "Lower quartile (25%) is: 0.004120528997963601 and Upper Quartile (75%) is: 0.015020308976719\n",
      "iqr: 0.0108997799787554\n",
      "Cut Off is: 0.0163496699681331\n",
      " Allocation rate per person Lower: -0.012229140970169498\n",
      " Allocation rate per person Upper: 0.0313699789448521\n",
      "For  Allocation rate per person,total outlier cases: 693\n",
      "\n",
      "Lower quartile (25%) is: 0.774308962762401 and Upper Quartile (75%) is: 0.8503828485419616\n",
      "iqr: 0.07607388577956053\n",
      "Cut Off is: 0.1141108286693408\n",
      " Working Capital to Total Assets Lower: 0.6601981340930603\n",
      " Working Capital to Total Assets Upper: 0.9644936772113024\n",
      "For  Working Capital to Total Assets,total outlier cases: 75\n",
      "\n",
      "Lower quartile (25%) is: 0.24197285659394002 and Upper Quartile (75%) is: 0.540593673285078\n",
      "iqr: 0.298620816691138\n",
      "Cut Off is: 0.447931225036707\n",
      " Quick Assets/Total Assets Lower: -0.205958368442767\n",
      " Quick Assets/Total Assets Upper: 0.988524898321785\n",
      "For  Quick Assets/Total Assets,total outlier cases: 2\n",
      "\n",
      "Lower quartile (25%) is: 0.35284541721511353 and Upper Quartile (75%) is: 0.6890506806831516\n",
      "iqr: 0.33620526346803803\n",
      "Cut Off is: 0.504307895202057\n",
      " Current Assets/Total Assets Lower: -0.15146247798694346\n",
      " Current Assets/Total Assets Upper: 1.1933585758852086\n",
      "For  Current Assets/Total Assets,total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.03354322123979425 and Upper Quartile (75%) is: 0.1610731518633315\n",
      "iqr: 0.12752993062353724\n",
      "Cut Off is: 0.19129489593530585\n",
      " Cash/Total Assets Lower: -0.1577516746955116\n",
      " Cash/Total Assets Upper: 0.35236804779863734\n",
      "For  Cash/Total Assets,total outlier cases: 496\n",
      "\n",
      "Lower quartile (25%) is: 0.00523977582664085 and Upper Quartile (75%) is: 0.0129509103075746\n",
      "iqr: 0.007711134480933751\n",
      "Cut Off is: 0.011566701721400625\n",
      " Quick Assets/Current Liability Lower: -0.0063269258947597756\n",
      " Quick Assets/Current Liability Upper: 0.024517612028975226\n",
      "For  Quick Assets/Current Liability,total outlier cases: 596\n",
      "\n",
      "Lower quartile (25%) is: 0.0019730075415488497 and Upper Quartile (75%) is: 0.0128055731079178\n",
      "iqr: 0.01083256556636895\n",
      "Cut Off is: 0.016248848349553424\n",
      " Cash/Current Liability Lower: -0.014275840808004574\n",
      " Cash/Current Liability Upper: 0.029054421457471225\n",
      "For  Cash/Current Liability,total outlier cases: 728\n",
      "\n",
      "Lower quartile (25%) is: 0.0533012764320206 and Upper Quartile (75%) is: 0.1195229934695275\n",
      "iqr: 0.0662217170375069\n",
      "Cut Off is: 0.09933257555626035\n",
      " Current Liability to Assets Lower: -0.04603129912423975\n",
      " Current Liability to Assets Upper: 0.21885556902578784\n",
      "For  Current Liability to Assets,total outlier cases: 95\n",
      "\n",
      "Lower quartile (25%) is: 0.34102297735578047 and Upper Quartile (75%) is: 0.3609148870133705\n",
      "iqr: 0.019891909657590023\n",
      "Cut Off is: 0.029837864486385035\n",
      " Operating Funds to Liability Lower: 0.31118511286939543\n",
      " Operating Funds to Liability Upper: 0.3907527514997555\n",
      "For  Operating Funds to Liability,total outlier cases: 657\n",
      "\n",
      "Lower quartile (25%) is: 0.2770339694810945 and Upper Quartile (75%) is: 0.2774287054274715\n",
      "iqr: 0.0003947359463770117\n",
      "Cut Off is: 0.0005921039195655176\n",
      " Inventory/Working Capital Lower: 0.27644186556152894\n",
      " Inventory/Working Capital Upper: 0.278020809347037\n",
      "For  Inventory/Working Capital,total outlier cases: 944\n",
      "\n",
      "Lower quartile (25%) is: 0.0031631476746991002 and Upper Quartile (75%) is: 0.011146766748190151\n",
      "iqr: 0.007983619073491051\n",
      "Cut Off is: 0.011975428610236576\n",
      " Inventory/Current Liability Lower: -0.008812280935537476\n",
      " Inventory/Current Liability Upper: 0.023122195358426727\n",
      "For  Inventory/Current Liability,total outlier cases: 426\n",
      "\n",
      "Lower quartile (25%) is: 0.6269807662218725 and Upper Quartile (75%) is: 0.942026693700069\n",
      "iqr: 0.3150459274781965\n",
      "Cut Off is: 0.47256889121729473\n",
      " Current Liabilities/Liability Lower: 0.15441187500457776\n",
      " Current Liabilities/Liability Upper: 1.4145955849173637\n",
      "For  Current Liabilities/Liability,total outlier cases: 40\n",
      "\n",
      "Lower quartile (25%) is: 0.733611818564342 and Upper Quartile (75%) is: 0.738559910578823\n",
      "iqr: 0.004948092014480987\n",
      "Cut Off is: 0.0074221380217214805\n",
      " Working Capital/Equity Lower: 0.7261896805426205\n",
      " Working Capital/Equity Upper: 0.7459820486005444\n",
      "For  Working Capital/Equity,total outlier cases: 153\n",
      "\n",
      "Lower quartile (25%) is: 0.328095841686878 and Upper Quartile (75%) is: 0.332322404809702\n",
      "iqr: 0.00422656312282399\n",
      "Cut Off is: 0.006339844684235985\n",
      " Current Liabilities/Equity Lower: 0.321755997002642\n",
      " Current Liabilities/Equity Upper: 0.338662249493938\n",
      "For  Current Liabilities/Equity,total outlier cases: 480\n",
      "\n",
      "Lower quartile (25%) is: 0.0 and Upper Quartile (75%) is: 0.009005945944256601\n",
      "iqr: 0.009005945944256601\n",
      "Cut Off is: 0.013508918916384902\n",
      " Long-term Liability to Current Assets Lower: -0.013508918916384902\n",
      " Long-term Liability to Current Assets Upper: 0.022514864860641505\n",
      "For  Long-term Liability to Current Assets,total outlier cases: 620\n",
      "\n",
      "Lower quartile (25%) is: 0.9310965081459854 and Upper Quartile (75%) is: 0.9448112860939986\n",
      "iqr: 0.013714777948013124\n",
      "Cut Off is: 0.020572166922019686\n",
      " Retained Earnings to Total Assets Lower: 0.9105243412239657\n",
      " Retained Earnings to Total Assets Upper: 0.9653834530160182\n",
      "For  Retained Earnings to Total Assets,total outlier cases: 633\n",
      "\n",
      "Lower quartile (25%) is: 0.0022355962096577498 and Upper Quartile (75%) is: 0.0024918511193838\n",
      "iqr: 0.00025625490972605016\n",
      "Cut Off is: 0.00038438236458907525\n",
      " Total income/Total expense Lower: 0.0018512138450686745\n",
      " Total income/Total expense Upper: 0.0028762334839728754\n",
      "For  Total income/Total expense,total outlier cases: 463\n",
      "\n",
      "Lower quartile (25%) is: 0.01456705658927065 and Upper Quartile (75%) is: 0.035930137895265155\n",
      "iqr: 0.021363081305994507\n",
      "Cut Off is: 0.032044621958991756\n",
      " Total expense/Assets Lower: -0.01747756536972111\n",
      " Total expense/Assets Upper: 0.06797475985425691\n",
      "For  Total expense/Assets,total outlier cases: 372\n",
      "\n",
      "Lower quartile (25%) is: 0.00014562362973865 and Upper Quartile (75%) is: 0.0004525945407579\n",
      "iqr: 0.00030697091101925\n",
      "Cut Off is: 0.000460456366528875\n",
      " Current Asset Turnover Rate Lower: -0.000314832736790225\n",
      " Current Asset Turnover Rate Upper: 0.000913050907286775\n",
      "For  Current Asset Turnover Rate,total outlier cases: 1399\n",
      "\n",
      "Lower quartile (25%) is: 0.00014171486236355001 and Upper Quartile (75%) is: 4900000000.0\n",
      "iqr: 4899999999.999858\n",
      "Cut Off is: 7349999999.999786\n",
      " Quick Asset Turnover Rate Lower: -7349999999.999644\n",
      " Quick Asset Turnover Rate Upper: 12249999999.999786\n",
      "For  Quick Asset Turnover Rate,total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.5939344215587965 and Upper Quartile (75%) is: 0.5940023454696105\n",
      "iqr: 6.792391081400506e-05\n",
      "Cut Off is: 0.00010188586622100759\n",
      " Working capitcal Turnover Rate Lower: 0.5938325356925755\n",
      " Working capitcal Turnover Rate Upper: 0.5941042313358316\n",
      "For  Working capitcal Turnover Rate,total outlier cases: 578\n",
      "\n",
      "Lower quartile (25%) is: 0.0002735337396781 and Upper Quartile (75%) is: 4510000000.0\n",
      "iqr: 4509999999.999726\n",
      "Cut Off is: 6764999999.99959\n",
      " Cash Turnover Rate Lower: -6764999999.999316\n",
      " Cash Turnover Rate Upper: 11274999999.99959\n",
      "For  Cash Turnover Rate,total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.671565259253275 and Upper Quartile (75%) is: 0.671586580417158\n",
      "iqr: 2.132116388298133e-05\n",
      "Cut Off is: 3.1981745824471997e-05\n",
      " Cash Flow to Sales Lower: 0.6715332775074505\n",
      " Cash Flow to Sales Upper: 0.6716185621629824\n",
      "For  Cash Flow to Sales,total outlier cases: 1052\n",
      "\n",
      "Lower quartile (25%) is: 0.0853603651897917 and Upper Quartile (75%) is: 0.3721999782647555\n",
      "iqr: 0.28683961307496375\n",
      "Cut Off is: 0.4302594196124456\n",
      " Fixed Assets to Assets Lower: -0.3448990544226539\n",
      " Fixed Assets to Assets Upper: 0.8024593978772011\n",
      "For  Fixed Assets to Assets,total outlier cases: 62\n",
      "\n",
      "Lower quartile (25%) is: 0.6269807662218725 and Upper Quartile (75%) is: 0.942026693700069\n",
      "iqr: 0.3150459274781965\n",
      "Cut Off is: 0.47256889121729473\n",
      " Current Liability to Liability Lower: 0.15441187500457776\n",
      " Current Liability to Liability Upper: 1.4145955849173637\n",
      "For  Current Liability to Liability,total outlier cases: 40\n",
      "\n",
      "Lower quartile (25%) is: 0.328095841686878 and Upper Quartile (75%) is: 0.332322404809702\n",
      "iqr: 0.00422656312282399\n",
      "Cut Off is: 0.006339844684235985\n",
      " Current Liability to Equity Lower: 0.321755997002642\n",
      " Current Liability to Equity Upper: 0.338662249493938\n",
      "For  Current Liability to Equity,total outlier cases: 480\n",
      "\n",
      "Lower quartile (25%) is: 0.110933233663468 and Upper Quartile (75%) is: 0.117106091075626\n",
      "iqr: 0.006172857412158006\n",
      "Cut Off is: 0.009259286118237009\n",
      " Equity to Long-term Liability Lower: 0.10167394754523099\n",
      " Equity to Long-term Liability Upper: 0.126365377193863\n",
      "For  Equity to Long-term Liability,total outlier cases: 406\n",
      "\n",
      "Lower quartile (25%) is: 0.633265319013864 and Upper Quartile (75%) is: 0.6630618534616091\n",
      "iqr: 0.029796534447745104\n",
      "Cut Off is: 0.04469480167161766\n",
      " Cash Flow to Total Assets Lower: 0.5885705173422463\n",
      " Cash Flow to Total Assets Upper: 0.7077566551332267\n",
      "For  Cash Flow to Total Assets,total outlier cases: 878\n",
      "\n",
      "Lower quartile (25%) is: 0.4571164765642225 and Upper Quartile (75%) is: 0.46423584697152853\n",
      "iqr: 0.007119370407306036\n",
      "Cut Off is: 0.010679055610959054\n",
      " Cash Flow to Liability Lower: 0.44643742095326344\n",
      " Cash Flow to Liability Upper: 0.4749149025824876\n",
      "For  Cash Flow to Liability,total outlier cases: 1212\n",
      "\n",
      "Lower quartile (25%) is: 0.5659869401753586 and Upper Quartile (75%) is: 0.6247688757833555\n",
      "iqr: 0.05878193560799694\n",
      "Cut Off is: 0.0881729034119954\n",
      " CFO to Assets Lower: 0.47781403676336315\n",
      " CFO to Assets Upper: 0.712941779195351\n",
      "For  CFO to Assets,total outlier cases: 342\n",
      "\n",
      "Lower quartile (25%) is: 0.312994699600273 and Upper Quartile (75%) is: 0.317707188742567\n",
      "iqr: 0.004712489142293996\n",
      "Cut Off is: 0.0070687337134409944\n",
      " Cash Flow to Equity Lower: 0.30592596588683196\n",
      " Cash Flow to Equity Upper: 0.32477592245600795\n",
      "For  Cash Flow to Equity,total outlier cases: 827\n",
      "\n",
      "Lower quartile (25%) is: 0.018033665707965 and Upper Quartile (75%) is: 0.0383746158541899\n",
      "iqr: 0.0203409501462249\n",
      "Cut Off is: 0.03051142521933735\n",
      " Current Liability to Current Assets Lower: -0.012477759511372349\n",
      " Current Liability to Current Assets Upper: 0.06888604107352725\n",
      "For  Current Liability to Current Assets,total outlier cases: 276\n",
      "\n",
      "Lower quartile (25%) is: 0.0 and Upper Quartile (75%) is: 0.0\n",
      "iqr: 0.0\n",
      "Cut Off is: 0.0\n",
      " Liability-Assets Flag Lower: 0.0\n",
      " Liability-Assets Flag Upper: 0.0\n",
      "For  Liability-Assets Flag,total outlier cases: 8\n",
      "\n",
      "Lower quartile (25%) is: 0.7967498491931705 and Upper Quartile (75%) is: 0.8264545295408715\n",
      "iqr: 0.029704680347701018\n",
      "Cut Off is: 0.04455702052155153\n",
      " Net Income to Total Assets Lower: 0.752192828671619\n",
      " Net Income to Total Assets Upper: 0.871011550062423\n",
      "For  Net Income to Total Assets,total outlier cases: 561\n",
      "\n",
      "Lower quartile (25%) is: 0.0009036204813306 and Upper Quartile (75%) is: 0.0052697768568805\n",
      "iqr: 0.0043661563755499\n",
      "Cut Off is: 0.00654923456332485\n",
      " Total assets to GNP price Lower: -0.005645614081994249\n",
      " Total assets to GNP price Upper: 0.01181901142020535\n",
      "For  Total assets to GNP price,total outlier cases: 797\n",
      "\n",
      "Lower quartile (25%) is: 0.623636304973909 and Upper Quartile (75%) is: 0.6241681927893561\n",
      "iqr: 0.0005318878154471074\n",
      "Cut Off is: 0.0007978317231706611\n",
      " No-credit Interval Lower: 0.6228384732507384\n",
      " No-credit Interval Upper: 0.6249660245125268\n",
      "For  No-credit Interval,total outlier cases: 1139\n",
      "\n",
      "Lower quartile (25%) is: 0.6004428952063054 and Upper Quartile (75%) is: 0.613913271038147\n",
      "iqr: 0.013470375831841519\n",
      "Cut Off is: 0.02020556374776228\n",
      " Gross Profit to Sales Lower: 0.5802373314585432\n",
      " Gross Profit to Sales Upper: 0.6341188347859092\n",
      "For  Gross Profit to Sales,total outlier cases: 320\n",
      "\n",
      "Lower quartile (25%) is: 0.8401148040637195 and Upper Quartile (75%) is: 0.8423569700412374\n",
      "iqr: 0.002242165977517918\n",
      "Cut Off is: 0.003363248966276877\n",
      " Net Income to Stockholder's Equity Lower: 0.8367515550974427\n",
      " Net Income to Stockholder's Equity Upper: 0.8457202190075144\n",
      "For  Net Income to Stockholder's Equity,total outlier cases: 571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lower quartile (25%) is: 0.276944242646329 and Upper Quartile (75%) is: 0.2814491856088265\n",
      "iqr: 0.004504942962497482\n",
      "Cut Off is: 0.006757414443746224\n",
      " Liability to Equity Lower: 0.2701868282025828\n",
      " Liability to Equity Upper: 0.2882066000525727\n",
      "For  Liability to Equity,total outlier cases: 404\n",
      "\n",
      "Lower quartile (25%) is: 0.0267911566924924 and Upper Quartile (75%) is: 0.026913184214613348\n",
      "iqr: 0.00012202752212094742\n",
      "Cut Off is: 0.00018304128318142113\n",
      " Degree of Financial Leverage (DFL) Lower: 0.02660811540931098\n",
      " Degree of Financial Leverage (DFL) Upper: 0.02709622549779477\n",
      "For  Degree of Financial Leverage (DFL),total outlier cases: 1503\n",
      "\n",
      "Lower quartile (25%) is: 0.565158395757604 and Upper Quartile (75%) is: 0.565724709506105\n",
      "iqr: 0.0005663137485010239\n",
      "Cut Off is: 0.0008494706227515358\n",
      " Interest Coverage Ratio (Interest expense to EBIT) Lower: 0.5643089251348524\n",
      " Interest Coverage Ratio (Interest expense to EBIT) Upper: 0.5665741801288565\n",
      "For  Interest Coverage Ratio (Interest expense to EBIT),total outlier cases: 1421\n",
      "\n",
      "Lower quartile (25%) is: 1.0 and Upper Quartile (75%) is: 1.0\n",
      "iqr: 0.0\n",
      "Cut Off is: 0.0\n",
      " Net Income Flag Lower: 1.0\n",
      " Net Income Flag Upper: 1.0\n",
      "For  Net Income Flag,total outlier cases: 0\n",
      "\n",
      "Lower quartile (25%) is: 0.024476693570910098 and Upper Quartile (75%) is: 0.052837817459331596\n",
      "iqr: 0.028361123888421498\n",
      "Cut Off is: 0.04254168583263225\n",
      " Equity to Liability Lower: -0.018064992261722153\n",
      " Equity to Liability Upper: 0.09537950329196385\n",
      "For  Equity to Liability,total outlier cases: 549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def outliers_treated(variable,variable_name,df):\n",
    "    \n",
    "    # 25th & 75th quartiles of the data\n",
    "    low_qrt = np.percentile(variable, 25)\n",
    "    up_qrt = np.percentile(variable, 75)\n",
    "    print(f'Lower quartile (25%) is: {low_qrt} and Upper Quartile (75%) is: {up_qrt}')\n",
    "    iqr_qrt = up_qrt - low_qrt\n",
    "    print(f'iqr: {iqr_qrt}')\n",
    "    \n",
    "    cut_off_var = iqr_qrt * 1.5\n",
    "    lower_var, upper_var = low_qrt - cut_off_var, up_qrt + cut_off_var\n",
    "    print(f'Cut Off is: {cut_off_var}')\n",
    "    print(variable_name +' Lower: {}'.format(lower_var))\n",
    "    print(variable_name +' Upper: {}'.format(upper_var))\n",
    "    \n",
    "    outliers = [val for val in variable if val < lower_var or val > upper_var]\n",
    "    print(f'For {variable_name},total outlier cases: {len(outliers)}')\n",
    "\n",
    "    df = df.drop(df[(df[variable_name] > upper_var) | (df[variable_name] < lower_var)].index)\n",
    "    print(\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "for column in company:\n",
    "    company_upd = outliers_treated(company[column],str(column),company)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b596c9",
   "metadata": {},
   "source": [
    "<br id=\"3\">\n",
    "<a href=\"#index\"  style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 3. Training and Scaling Data </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfcfd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = company_upd.drop('Bankrupt?', axis=1)\n",
    "y = company_upd['Bankrupt?']\n",
    "\n",
    "# Select the binary columns from the original dataframe\n",
    "binary_cols = [' Liability-Assets Flag', ' Net Income Flag']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "# Reset the index of the original dataframe and the scaled features\n",
    "X_train_reset = X_train.reset_index(drop=True) \n",
    "\n",
    "X_train_sc = pd.DataFrame(scale.fit_transform(X_train.drop(binary_cols, axis=1)), columns=X_train.drop(binary_cols,\n",
    "                                                                                                       axis=1).columns)\n",
    "X_train_scaled = X_train_sc.reset_index(drop=True)\n",
    "\n",
    "X_train = pd.concat([X_train_scaled, X_train_reset[binary_cols]], axis=1)\n",
    "\n",
    "# Reset the index of the original dataframe and the scaled features\n",
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "\n",
    "X_test_sc = pd.DataFrame(scale.transform(X_test.drop(binary_cols, axis=1)), columns=X_test.drop(binary_cols, axis=1)\n",
    "                         .columns)\n",
    "X_test_scaled = X_test_sc.reset_index(drop=True)\n",
    "\n",
    "X_test = pd.concat([X_test_scaled, X_test_reset[binary_cols]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5391b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()\n",
    "\n",
    "#NaNs check\n",
    "[print(col) for col in X_train if X_train[col].isna().sum() > 0]\n",
    "\n",
    "X_test.isna().sum()\n",
    "\n",
    "#NaNs check\n",
    "[print(col) for col in X_test if X_test[col].isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442354ff",
   "metadata": {},
   "source": [
    "<br id=\"4\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 4. SMOTE Oversampling  </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd4c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform oversampling on the training data using SMOTE\n",
    "\n",
    "sm1 = SMOTE(random_state=42)\n",
    "X_train_bal1, y_train_bal1 = sm1.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a1bb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4228\n",
      "1    4228\n",
      "Name: Bankrupt?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_counts = y_train_bal1.value_counts()\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d0f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8456, 95), (1881, 95), (8456,), (1881,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal1.shape,X_test.shape,y_train_bal1.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59a4a5",
   "metadata": {},
   "source": [
    "<br id=\"5\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 5. Baseline Model: Logistic Regression </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c070b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8904837852206273\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      1824\n",
      "           1       0.17      0.68      0.27        57\n",
      "\n",
      "    accuracy                           0.89      1881\n",
      "   macro avg       0.58      0.79      0.61      1881\n",
      "weighted avg       0.96      0.89      0.92      1881\n",
      "\n",
      "[[1636  188]\n",
      " [  18   39]]\n",
      "\n",
      "F1 Score  0.2746478873239437\n",
      "Balanced Accuracy Score  0.7905701754385965\n"
     ]
    }
   ],
   "source": [
    "#CELL 0\n",
    "\n",
    "logreg3 = LogisticRegression()\n",
    "logreg3.fit(X_train_bal1, y_train_bal1)\n",
    "\n",
    "# Predict on the test set\n",
    "logpred3 = logreg3.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(y_test, logpred3)\n",
    "report = classification_report(y_test, logpred3)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Classification Report:\\n', report)\n",
    "print(confusion_matrix(y_test, logpred3))\n",
    "print(\"\")\n",
    "print(\"F1 Score \",f1_score(y_test, logpred3))\n",
    "print(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, logpred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac900a",
   "metadata": {},
   "source": [
    "<br id=\"6\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 6. Model 1: 5-layer-ANN-SMOTE </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cab449cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    P = K.sum(K.round(K.clip(y_true, 0, 1))) #what we call all truly possible positives: TP+FN\n",
    "    P_ = K.sum(K.round(K.clip(y_pred, 0, 1))) #what we call all predicted positives: TP+FP\n",
    "    precision = TP / (P_ + K.epsilon())\n",
    "    recall = TP / (P + K.epsilon())\n",
    "    f1_value = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8840b2",
   "metadata": {},
   "source": [
    "<br id=\"6.1\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">6.1 Tuning the hyperparameters</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35cdc43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 1s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.4241   \u001b[0m | \u001b[0m5.51     \u001b[0m | \u001b[0m335.3    \u001b[0m | \u001b[0m54.88    \u001b[0m | \u001b[0m0.7716   \u001b[0m | \u001b[0m36.58    \u001b[0m | \u001b[0m1.044    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.4757   \u001b[0m | \u001b[95m0.2023   \u001b[0m | \u001b[95m536.2    \u001b[0m | \u001b[95m39.09    \u001b[0m | \u001b[95m0.3443   \u001b[0m | \u001b[95m99.16    \u001b[0m | \u001b[95m1.664    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 1s 1ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2666   \u001b[0m | \u001b[0m0.7307   \u001b[0m | \u001b[0m735.7    \u001b[0m | \u001b[0m69.7     \u001b[0m | \u001b[0m0.2815   \u001b[0m | \u001b[0m51.96    \u001b[0m | \u001b[0m0.8286   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.3999   \u001b[0m | \u001b[0m0.6656   \u001b[0m | \u001b[0m920.6    \u001b[0m | \u001b[0m83.52    \u001b[0m | \u001b[0m0.8422   \u001b[0m | \u001b[0m83.37    \u001b[0m | \u001b[0m6.937    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 1s 8ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.9778   \u001b[0m | \u001b[95m5.195    \u001b[0m | \u001b[95m851.0    \u001b[0m | \u001b[95m53.71    \u001b[0m | \u001b[95m0.03717  \u001b[0m | \u001b[95m50.87    \u001b[0m | \u001b[95m0.7373   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.355    \u001b[0m | \u001b[0m758.2    \u001b[0m | \u001b[0m65.22    \u001b[0m | \u001b[0m0.2815   \u001b[0m | \u001b[0m99.86    \u001b[0m | \u001b[0m0.9663   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 6ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.4025   \u001b[0m | \u001b[0m5.539    \u001b[0m | \u001b[0m588.0    \u001b[0m | \u001b[0m52.4     \u001b[0m | \u001b[0m0.7306   \u001b[0m | \u001b[0m39.05    \u001b[0m | \u001b[0m2.804    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 5ms/step\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8624   \u001b[0m | \u001b[0m2.871    \u001b[0m | \u001b[0m957.8    \u001b[0m | \u001b[0m93.5     \u001b[0m | \u001b[0m0.8157   \u001b[0m | \u001b[0m13.07    \u001b[0m | \u001b[0m6.604    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 5ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.5797   \u001b[0m | \u001b[0m8.554    \u001b[0m | \u001b[0m845.3    \u001b[0m | \u001b[0m58.5     \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m47.53    \u001b[0m | \u001b[0m2.232    \u001b[0m |\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 1s 7ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9148   \u001b[0m | \u001b[0m0.148    \u001b[0m | \u001b[0m230.5    \u001b[0m | \u001b[0m24.25    \u001b[0m | \u001b[0m0.1367   \u001b[0m | \u001b[0m13.0     \u001b[0m | \u001b[0m1.585    \u001b[0m |\n",
      "53/53 [==============================] - 1s 9ms/step\n",
      "53/53 [==============================] - 1s 6ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8267   \u001b[0m | \u001b[0m4.895    \u001b[0m | \u001b[0m342.9    \u001b[0m | \u001b[0m34.35    \u001b[0m | \u001b[0m0.1581   \u001b[0m | \u001b[0m71.47    \u001b[0m | \u001b[0m3.283    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.914    \u001b[0m | \u001b[0m735.1    \u001b[0m | \u001b[0m55.3     \u001b[0m | \u001b[0m0.5993   \u001b[0m | \u001b[0m51.55    \u001b[0m | \u001b[0m6.743    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.2667   \u001b[0m | \u001b[0m1.33     \u001b[0m | \u001b[0m925.5    \u001b[0m | \u001b[0m59.83    \u001b[0m | \u001b[0m0.5966   \u001b[0m | \u001b[0m71.62    \u001b[0m | \u001b[0m1.242    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9536   \u001b[0m | \u001b[0m7.782    \u001b[0m | \u001b[0m585.7    \u001b[0m | \u001b[0m25.55    \u001b[0m | \u001b[0m0.3711   \u001b[0m | \u001b[0m42.54    \u001b[0m | \u001b[0m3.304    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 1s 8ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.6479   \u001b[0m | \u001b[0m1.615    \u001b[0m | \u001b[0m340.2    \u001b[0m | \u001b[0m95.93    \u001b[0m | \u001b[0m0.6591   \u001b[0m | \u001b[0m22.15    \u001b[0m | \u001b[0m6.495    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9068   \u001b[0m | \u001b[0m7.576    \u001b[0m | \u001b[0m242.2    \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m0.8738   \u001b[0m | \u001b[0m70.65    \u001b[0m | \u001b[0m2.081    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.61     \u001b[0m | \u001b[0m694.7    \u001b[0m | \u001b[0m36.84    \u001b[0m | \u001b[0m0.804    \u001b[0m | \u001b[0m15.32    \u001b[0m | \u001b[0m2.158    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5998   \u001b[0m | \u001b[0m1.866    \u001b[0m | \u001b[0m977.8    \u001b[0m | \u001b[0m92.75    \u001b[0m | \u001b[0m0.6797   \u001b[0m | \u001b[0m20.37    \u001b[0m | \u001b[0m6.706    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.1333   \u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m703.8    \u001b[0m | \u001b[0m92.23    \u001b[0m | \u001b[0m0.3464   \u001b[0m | \u001b[0m68.75    \u001b[0m | \u001b[0m6.476    \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.887    \u001b[0m | \u001b[0m3.366    \u001b[0m | \u001b[0m817.1    \u001b[0m | \u001b[0m91.69    \u001b[0m | \u001b[0m0.624    \u001b[0m | \u001b[0m23.6     \u001b[0m | \u001b[0m2.624    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.461    \u001b[0m | \u001b[0m5.723    \u001b[0m | \u001b[0m567.3    \u001b[0m | \u001b[0m62.58    \u001b[0m | \u001b[0m0.3588   \u001b[0m | \u001b[0m69.39    \u001b[0m | \u001b[0m3.336    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.8395   \u001b[0m | \u001b[0m4.091    \u001b[0m | \u001b[0m299.8    \u001b[0m | \u001b[0m53.0     \u001b[0m | \u001b[0m0.2804   \u001b[0m | \u001b[0m41.21    \u001b[0m | \u001b[0m6.821    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.1334   \u001b[0m | \u001b[0m1.94     \u001b[0m | \u001b[0m746.3    \u001b[0m | \u001b[0m22.54    \u001b[0m | \u001b[0m0.837    \u001b[0m | \u001b[0m73.15    \u001b[0m | \u001b[0m6.762    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9195   \u001b[0m | \u001b[0m5.326    \u001b[0m | \u001b[0m373.9    \u001b[0m | \u001b[0m77.54    \u001b[0m | \u001b[0m0.04056  \u001b[0m | \u001b[0m47.68    \u001b[0m | \u001b[0m1.969    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.4      \u001b[0m | \u001b[0m0.9562   \u001b[0m | \u001b[0m541.1    \u001b[0m | \u001b[0m87.25    \u001b[0m | \u001b[0m0.1193   \u001b[0m | \u001b[0m98.8     \u001b[0m | \u001b[0m1.633    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.4318   \u001b[0m | \u001b[0m5.764    \u001b[0m | \u001b[0m854.0    \u001b[0m | \u001b[0m48.51    \u001b[0m | \u001b[0m0.5396   \u001b[0m | \u001b[0m60.91    \u001b[0m | \u001b[0m3.159    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.6522   \u001b[0m | \u001b[0m3.889    \u001b[0m | \u001b[0m343.3    \u001b[0m | \u001b[0m38.74    \u001b[0m | \u001b[0m0.6817   \u001b[0m | \u001b[0m71.78    \u001b[0m | \u001b[0m3.48     \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.976    \u001b[0m | \u001b[0m590.9    \u001b[0m | \u001b[0m20.0     \u001b[0m | \u001b[0m0.519    \u001b[0m | \u001b[0m43.53    \u001b[0m | \u001b[0m4.323    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.26     \u001b[0m | \u001b[0m584.6    \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m0.8535   \u001b[0m | \u001b[0m41.24    \u001b[0m | \u001b[0m0.6165   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Make scorer accuracy\n",
    "f1_scorer = make_scorer(fbeta_score, beta=1)\n",
    "\n",
    "def hypertuning_func1(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    \n",
    "    act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu','elu', 'exponential', LeakyReLU,'relu']\n",
    "    \n",
    "    neurons = round(neurons)\n",
    "    activation = act_list[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    \n",
    "    opt_list = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    \n",
    "    opt_dict= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    \n",
    "    \n",
    "    def temp_model1():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        model1 = Sequential()\n",
    "        model1.add(Dense(neurons, input_dim=95, activation=activation))\n",
    "        model1.add(Dense(neurons, activation=activation))\n",
    "        model1.add(Dense(neurons, activation=activation))\n",
    "        model1.add(Dense(neurons, activation=activation))\n",
    "        model1.add(Dense(1, activation='sigmoid')) #this is the output layer where we only have 1 neuron (this is always sigmoid)\n",
    "        model1.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "        return model1\n",
    "    \n",
    "    model1 = KerasClassifier(build_fn=temp_model1, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0) #creates a Stratifiedstrat_fold object converting data into folds for cross-validation\n",
    "    \n",
    "    strat_fold = Stratifiedstrat_fold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    score = cross_val_score(model1, X_train_bal1, y_train_bal1, scoring=f1_scorer, cv=strat_fold).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Set paramaters\n",
    "params_range ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "baye_opt1 = BayesianOptimization(hypertuning_func1, params_range, random_state=111)\n",
    "baye_opt1.maximize(init_points=25, n_iter=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dde34c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'selu',\n",
       " 'batch_size': 851.0135336291902,\n",
       " 'epochs': 53.7054301919375,\n",
       " 'learning_rate': 0.037173480215022196,\n",
       " 'neurons': 50.872297884262295,\n",
       " 'optimizer': 0.7372825972056519}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params1 = baye_opt1.max['params']\n",
    "act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU,'relu']\n",
    "\n",
    "tuned_params1['activation'] = act_list[round(tuned_params1['activation'])]\n",
    "tuned_params1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb928f28",
   "metadata": {},
   "source": [
    "<br id=\"6.2\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">6.2 Building the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54bf5c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/54\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 2.6260 - get_f1: 0.6008 - val_loss: 1.4456 - val_get_f1: 0.1469\n",
      "Epoch 2/54\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.4218 - get_f1: 0.8623 - val_loss: 0.3597 - val_get_f1: 0.2276\n",
      "Epoch 3/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2561 - get_f1: 0.9079 - val_loss: 0.2359 - val_get_f1: 0.2400\n",
      "Epoch 4/54\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.2122 - get_f1: 0.9272 - val_loss: 0.2239 - val_get_f1: 0.2915\n",
      "Epoch 5/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.1855 - get_f1: 0.9382 - val_loss: 0.2364 - val_get_f1: 0.3015\n",
      "Epoch 6/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.1698 - get_f1: 0.9467 - val_loss: 0.2219 - val_get_f1: 0.3386\n",
      "Epoch 7/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.1564 - get_f1: 0.9500 - val_loss: 0.2033 - val_get_f1: 0.3322\n",
      "Epoch 8/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.1437 - get_f1: 0.9561 - val_loss: 0.2277 - val_get_f1: 0.3199\n",
      "Epoch 9/54\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.1315 - get_f1: 0.9604 - val_loss: 0.2175 - val_get_f1: 0.3225\n",
      "Epoch 10/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.1149 - get_f1: 0.9669 - val_loss: 0.2043 - val_get_f1: 0.3447\n",
      "Epoch 11/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.1012 - get_f1: 0.9701 - val_loss: 0.2048 - val_get_f1: 0.3668\n",
      "Epoch 12/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.1027 - get_f1: 0.9692 - val_loss: 0.1830 - val_get_f1: 0.3311\n",
      "Epoch 13/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0916 - get_f1: 0.9738 - val_loss: 0.2773 - val_get_f1: 0.3338\n",
      "Epoch 14/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.1081 - get_f1: 0.9698 - val_loss: 0.2428 - val_get_f1: 0.3533\n",
      "Epoch 15/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0890 - get_f1: 0.9738 - val_loss: 0.2025 - val_get_f1: 0.3235\n",
      "Epoch 16/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0730 - get_f1: 0.9794 - val_loss: 0.1857 - val_get_f1: 0.2292\n",
      "Epoch 17/54\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.0702 - get_f1: 0.9792 - val_loss: 0.1999 - val_get_f1: 0.2444\n",
      "Epoch 18/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0545 - get_f1: 0.9860 - val_loss: 0.2324 - val_get_f1: 0.2345\n",
      "Epoch 19/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0496 - get_f1: 0.9872 - val_loss: 0.2460 - val_get_f1: 0.3558\n",
      "Epoch 20/54\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0450 - get_f1: 0.9884 - val_loss: 0.2268 - val_get_f1: 0.3252\n",
      "Epoch 21/54\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0380 - get_f1: 0.9901 - val_loss: 0.2530 - val_get_f1: 0.2277\n",
      "Epoch 22/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0333 - get_f1: 0.9921 - val_loss: 0.2538 - val_get_f1: 0.3399\n",
      "Epoch 23/54\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 0.0480 - get_f1: 0.9847 - val_loss: 0.2682 - val_get_f1: 0.2062\n",
      "Epoch 24/54\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0794 - get_f1: 0.9759 - val_loss: 0.2445 - val_get_f1: 0.3406\n",
      "Epoch 25/54\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0671 - get_f1: 0.9816 - val_loss: 0.2451 - val_get_f1: 0.3312\n",
      "Epoch 26/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0415 - get_f1: 0.9888 - val_loss: 0.2843 - val_get_f1: 0.2984\n",
      "Epoch 27/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0427 - get_f1: 0.9879 - val_loss: 0.2892 - val_get_f1: 0.3060\n",
      "Epoch 28/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0351 - get_f1: 0.9893 - val_loss: 0.3263 - val_get_f1: 0.3737\n",
      "Epoch 29/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0260 - get_f1: 0.9928 - val_loss: 0.2979 - val_get_f1: 0.3377\n",
      "Epoch 30/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0208 - get_f1: 0.9952 - val_loss: 0.3210 - val_get_f1: 0.3248\n",
      "Epoch 31/54\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0161 - get_f1: 0.9960 - val_loss: 0.3562 - val_get_f1: 0.3111\n",
      "Epoch 32/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0170 - get_f1: 0.9959 - val_loss: 0.3624 - val_get_f1: 0.3405\n",
      "Epoch 33/54\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0121 - get_f1: 0.9972 - val_loss: 0.3566 - val_get_f1: 0.3322\n",
      "Epoch 34/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0116 - get_f1: 0.9977 - val_loss: 0.3774 - val_get_f1: 0.3428\n",
      "Epoch 35/54\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0101 - get_f1: 0.9978 - val_loss: 0.3926 - val_get_f1: 0.3265\n",
      "Epoch 36/54\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0072 - get_f1: 0.9982 - val_loss: 0.4080 - val_get_f1: 0.3400\n",
      "Epoch 37/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0067 - get_f1: 0.9989 - val_loss: 0.4152 - val_get_f1: 0.3570\n",
      "Epoch 38/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0067 - get_f1: 0.9987 - val_loss: 0.4230 - val_get_f1: 0.3388\n",
      "Epoch 39/54\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0069 - get_f1: 0.9982 - val_loss: 0.4346 - val_get_f1: 0.3627\n",
      "Epoch 40/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0062 - get_f1: 0.9988 - val_loss: 0.4329 - val_get_f1: 0.3333\n",
      "Epoch 41/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0063 - get_f1: 0.9987 - val_loss: 0.4654 - val_get_f1: 0.3301\n",
      "Epoch 42/54\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0058 - get_f1: 0.9986 - val_loss: 0.4303 - val_get_f1: 0.3631\n",
      "Epoch 43/54\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0050 - get_f1: 0.9991 - val_loss: 0.4302 - val_get_f1: 0.3493\n",
      "Epoch 44/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0050 - get_f1: 0.9990 - val_loss: 0.4659 - val_get_f1: 0.3375\n",
      "Epoch 45/54\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0184 - get_f1: 0.9964 - val_loss: 0.4554 - val_get_f1: 0.3615\n",
      "Epoch 46/54\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0287 - get_f1: 0.9933 - val_loss: 0.4549 - val_get_f1: 0.3193\n",
      "Epoch 47/54\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0340 - get_f1: 0.9887 - val_loss: 0.4951 - val_get_f1: 0.3088\n",
      "Epoch 48/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.1593 - get_f1: 0.9643 - val_loss: 0.6447 - val_get_f1: 0.3087\n",
      "Epoch 49/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2682 - get_f1: 0.9467 - val_loss: 0.5525 - val_get_f1: 0.2638\n",
      "Epoch 50/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.1571 - get_f1: 0.9581 - val_loss: 0.4218 - val_get_f1: 0.2544\n",
      "Epoch 51/54\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0835 - get_f1: 0.9739 - val_loss: 0.3361 - val_get_f1: 0.3206\n",
      "Epoch 52/54\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0551 - get_f1: 0.9833 - val_loss: 0.4169 - val_get_f1: 0.2489\n",
      "Epoch 53/54\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0485 - get_f1: 0.9858 - val_loss: 0.3825 - val_get_f1: 0.2372\n",
      "Epoch 54/54\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.0327 - get_f1: 0.9912 - val_loss: 0.4065 - val_get_f1: 0.3052\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(round(tuned_params1['neurons']), input_dim=95, activation=tuned_params1['activation']))\n",
    "nn.add(Dense(round(tuned_params1['neurons']), activation=tuned_params1['activation']))\n",
    "nn.add(Dense(round(tuned_params1['neurons']), activation=tuned_params1['activation']))\n",
    "\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "nn.compile(loss='binary_crossentropy', optimizer=Adam(lr = tuned_params1['learning_rate']), metrics=[get_f1])\n",
    "\n",
    "history = nn.fit(X_train_bal1, y_train_bal1,batch_size=round(tuned_params1['batch_size']), \n",
    "                        epochs=round(tuned_params1['epochs']), validation_data=(X_test,y_test))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5e6790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1824\n",
      "           1       0.29      0.40      0.34        57\n",
      "\n",
      "    accuracy                           0.95      1881\n",
      "   macro avg       0.64      0.69      0.66      1881\n",
      "weighted avg       0.96      0.95      0.96      1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model,x):\n",
    "    pred  = model.predict(x)\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    return pred\n",
    "\n",
    "def predict_graph(y_true,y_pred,title):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(cm,annot=True,fmt='g',cmap=\"YlGnBu\",\n",
    "                xticklabels=['No Bankruptcy','Bankruptcy'],yticklabels=['No Bankruptcy','Bankruptcy'])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true,y_pred))\n",
    "\n",
    "\n",
    "y_test_pred = predict(nn,X_test)\n",
    "predict_graph(y_test,y_test_pred,'Test Data Predictions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15acae5",
   "metadata": {},
   "source": [
    "<br id=\"7\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 7. Model 2: 6-layer-ANN-SMOTE </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0b55e",
   "metadata": {},
   "source": [
    "<br id=\"7.1\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">7.1 Tuning the hyperparameters</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f533975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2666   \u001b[0m | \u001b[0m5.51     \u001b[0m | \u001b[0m335.3    \u001b[0m | \u001b[0m0.4361   \u001b[0m | \u001b[0m0.2308   \u001b[0m | \u001b[0m43.63    \u001b[0m | \u001b[0m1.298    \u001b[0m | \u001b[0m1.045    \u001b[0m | \u001b[0m0.426    \u001b[0m | \u001b[0m31.48    \u001b[0m | \u001b[0m0.3377   \u001b[0m | \u001b[0m6.935    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.4015   \u001b[0m | \u001b[95m2.14     \u001b[0m | \u001b[95m265.0    \u001b[0m | \u001b[95m0.6696   \u001b[0m | \u001b[95m0.1864   \u001b[0m | \u001b[95m41.94    \u001b[0m | \u001b[95m1.932    \u001b[0m | \u001b[95m1.237    \u001b[0m | \u001b[95m0.08322  \u001b[0m | \u001b[95m91.07    \u001b[0m | \u001b[95m0.794    \u001b[0m | \u001b[95m5.884    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.337    \u001b[0m | \u001b[0m992.8    \u001b[0m | \u001b[0m0.5773   \u001b[0m | \u001b[0m0.2441   \u001b[0m | \u001b[0m53.71    \u001b[0m | \u001b[0m1.055    \u001b[0m | \u001b[0m1.908    \u001b[0m | \u001b[0m0.1143   \u001b[0m | \u001b[0m83.55    \u001b[0m | \u001b[0m0.6977   \u001b[0m | \u001b[0m3.957    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2666   \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m998.8    \u001b[0m | \u001b[0m0.138    \u001b[0m | \u001b[0m0.1846   \u001b[0m | \u001b[0m58.8     \u001b[0m | \u001b[0m1.81     \u001b[0m | \u001b[0m2.456    \u001b[0m | \u001b[0m0.3296   \u001b[0m | \u001b[0m46.05    \u001b[0m | \u001b[0m0.319    \u001b[0m | \u001b[0m6.631    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.9702   \u001b[0m | \u001b[95m8.268    \u001b[0m | \u001b[95m851.1    \u001b[0m | \u001b[95m0.03408  \u001b[0m | \u001b[95m0.283    \u001b[0m | \u001b[95m96.04    \u001b[0m | \u001b[95m2.613    \u001b[0m | \u001b[95m1.963    \u001b[0m | \u001b[95m0.9671   \u001b[0m | \u001b[95m47.53    \u001b[0m | \u001b[95m0.3188   \u001b[0m | \u001b[95m0.1151   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9461   \u001b[0m | \u001b[0m0.3436   \u001b[0m | \u001b[0m242.5    \u001b[0m | \u001b[0m0.128    \u001b[0m | \u001b[0m0.01001  \u001b[0m | \u001b[0m38.11    \u001b[0m | \u001b[0m2.088    \u001b[0m | \u001b[0m1.357    \u001b[0m | \u001b[0m0.1876   \u001b[0m | \u001b[0m23.47    \u001b[0m | \u001b[0m0.683    \u001b[0m | \u001b[0m3.283    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.914    \u001b[0m | \u001b[0m735.1    \u001b[0m | \u001b[0m0.4413   \u001b[0m | \u001b[0m0.1786   \u001b[0m | \u001b[0m56.93    \u001b[0m | \u001b[0m2.927    \u001b[0m | \u001b[0m1.296    \u001b[0m | \u001b[0m0.9077   \u001b[0m | \u001b[0m54.81    \u001b[0m | \u001b[0m0.5925   \u001b[0m | \u001b[0m4.793    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.9793   \u001b[0m | \u001b[95m1.597    \u001b[0m | \u001b[95m891.7    \u001b[0m | \u001b[95m0.4821   \u001b[0m | \u001b[95m0.0208   \u001b[0m | \u001b[95m49.18    \u001b[0m | \u001b[95m1.723    \u001b[0m | \u001b[95m1.944    \u001b[0m | \u001b[95m0.1877   \u001b[0m | \u001b[95m25.78    \u001b[0m | \u001b[95m0.9491   \u001b[0m | \u001b[95m4.59     \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.1334   \u001b[0m | \u001b[0m1.215    \u001b[0m | \u001b[0m942.2    \u001b[0m | \u001b[0m0.8418   \u001b[0m | \u001b[0m0.01583  \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m2.745    \u001b[0m | \u001b[0m2.348    \u001b[0m | \u001b[0m0.3043   \u001b[0m | \u001b[0m76.1     \u001b[0m | \u001b[0m0.6183   \u001b[0m | \u001b[0m1.473    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.219    \u001b[0m | \u001b[0m247.3    \u001b[0m | \u001b[0m0.3082   \u001b[0m | \u001b[0m0.06221  \u001b[0m | \u001b[0m97.78    \u001b[0m | \u001b[0m2.819    \u001b[0m | \u001b[0m2.353    \u001b[0m | \u001b[0m0.124    \u001b[0m | \u001b[0m96.22    \u001b[0m | \u001b[0m0.09171  \u001b[0m | \u001b[0m4.409    \u001b[0m |\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9666   \u001b[0m | \u001b[0m8.126    \u001b[0m | \u001b[0m471.8    \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.2775   \u001b[0m | \u001b[0m49.92    \u001b[0m | \u001b[0m2.543    \u001b[0m | \u001b[0m2.792    \u001b[0m | \u001b[0m0.624    \u001b[0m | \u001b[0m23.6     \u001b[0m | \u001b[0m0.3749   \u001b[0m | \u001b[0m4.451    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.2666   \u001b[0m | \u001b[0m4.132    \u001b[0m | \u001b[0m625.8    \u001b[0m | \u001b[0m0.3523   \u001b[0m | \u001b[0m0.198    \u001b[0m | \u001b[0m58.12    \u001b[0m | \u001b[0m1.909    \u001b[0m | \u001b[0m1.25     \u001b[0m | \u001b[0m0.4183   \u001b[0m | \u001b[0m34.58    \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m6.821    \u001b[0m |\n",
      "53/53 [==============================] - 1s 5ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9008   \u001b[0m | \u001b[0m1.94     \u001b[0m | \u001b[0m746.3    \u001b[0m | \u001b[0m0.03181  \u001b[0m | \u001b[0m0.2506   \u001b[0m | \u001b[0m76.13    \u001b[0m | \u001b[0m2.932    \u001b[0m | \u001b[0m2.184    \u001b[0m | \u001b[0m0.2252   \u001b[0m | \u001b[0m74.73    \u001b[0m | \u001b[0m0.03087  \u001b[0m | \u001b[0m2.931    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9113   \u001b[0m | \u001b[0m2.531    \u001b[0m | \u001b[0m285.0    \u001b[0m | \u001b[0m0.4263   \u001b[0m | \u001b[0m0.2522   \u001b[0m | \u001b[0m28.83    \u001b[0m | \u001b[0m2.973    \u001b[0m | \u001b[0m1.467    \u001b[0m | \u001b[0m0.7242   \u001b[0m | \u001b[0m69.48    \u001b[0m | \u001b[0m0.07776  \u001b[0m | \u001b[0m4.881    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9648   \u001b[0m | \u001b[0m2.388    \u001b[0m | \u001b[0m921.5    \u001b[0m | \u001b[0m0.8183   \u001b[0m | \u001b[0m0.1198   \u001b[0m | \u001b[0m85.62    \u001b[0m | \u001b[0m1.396    \u001b[0m | \u001b[0m2.045    \u001b[0m | \u001b[0m0.4184   \u001b[0m | \u001b[0m93.33    \u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m3.507    \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8897   \u001b[0m | \u001b[0m1.051    \u001b[0m | \u001b[0m209.3    \u001b[0m | \u001b[0m0.9132   \u001b[0m | \u001b[0m0.1537   \u001b[0m | \u001b[0m87.45    \u001b[0m | \u001b[0m1.19     \u001b[0m | \u001b[0m2.607    \u001b[0m | \u001b[0m0.07161  \u001b[0m | \u001b[0m67.19    \u001b[0m | \u001b[0m0.9688   \u001b[0m | \u001b[0m2.782    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.4001   \u001b[0m | \u001b[0m5.936    \u001b[0m | \u001b[0m371.9    \u001b[0m | \u001b[0m0.8899   \u001b[0m | \u001b[0m0.296    \u001b[0m | \u001b[0m79.09    \u001b[0m | \u001b[0m2.283    \u001b[0m | \u001b[0m1.504    \u001b[0m | \u001b[0m0.4811   \u001b[0m | \u001b[0m34.13    \u001b[0m | \u001b[0m0.8683   \u001b[0m | \u001b[0m1.868    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.4462   \u001b[0m | \u001b[0m8.757    \u001b[0m | \u001b[0m370.8    \u001b[0m | \u001b[0m0.2978   \u001b[0m | \u001b[0m0.221    \u001b[0m | \u001b[0m21.03    \u001b[0m | \u001b[0m1.06     \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m0.5033   \u001b[0m | \u001b[0m29.63    \u001b[0m | \u001b[0m0.00893  \u001b[0m | \u001b[0m5.955    \u001b[0m |\n",
      "53/53 [==============================] - 1s 6ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9772   \u001b[0m | \u001b[0m4.828    \u001b[0m | \u001b[0m778.8    \u001b[0m | \u001b[0m0.6616   \u001b[0m | \u001b[0m0.2516   \u001b[0m | \u001b[0m51.06    \u001b[0m | \u001b[0m1.852    \u001b[0m | \u001b[0m2.656    \u001b[0m | \u001b[0m0.4743   \u001b[0m | \u001b[0m83.8     \u001b[0m | \u001b[0m0.01418  \u001b[0m | \u001b[0m2.777    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.5334   \u001b[0m | \u001b[0m1.155    \u001b[0m | \u001b[0m294.5    \u001b[0m | \u001b[0m0.206    \u001b[0m | \u001b[0m0.2243   \u001b[0m | \u001b[0m94.41    \u001b[0m | \u001b[0m1.761    \u001b[0m | \u001b[0m1.921    \u001b[0m | \u001b[0m0.8746   \u001b[0m | \u001b[0m83.31    \u001b[0m | \u001b[0m0.02497  \u001b[0m | \u001b[0m6.111    \u001b[0m |\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.6109   \u001b[0m | \u001b[0m5.441    \u001b[0m | \u001b[0m613.2    \u001b[0m | \u001b[0m0.5893   \u001b[0m | \u001b[0m0.2399   \u001b[0m | \u001b[0m33.86    \u001b[0m | \u001b[0m1.374    \u001b[0m | \u001b[0m1.516    \u001b[0m | \u001b[0m0.06056  \u001b[0m | \u001b[0m59.74    \u001b[0m | \u001b[0m0.3518   \u001b[0m | \u001b[0m6.419    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.974    \u001b[0m | \u001b[0m4.289    \u001b[0m | \u001b[0m283.6    \u001b[0m | \u001b[0m0.1525   \u001b[0m | \u001b[0m0.08206  \u001b[0m | \u001b[0m82.52    \u001b[0m | \u001b[0m1.786    \u001b[0m | \u001b[0m2.598    \u001b[0m | \u001b[0m0.4387   \u001b[0m | \u001b[0m17.34    \u001b[0m | \u001b[0m0.01064  \u001b[0m | \u001b[0m3.016    \u001b[0m |\n",
      "53/53 [==============================] - 1s 8ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9573   \u001b[0m | \u001b[0m5.966    \u001b[0m | \u001b[0m612.2    \u001b[0m | \u001b[0m0.5801   \u001b[0m | \u001b[0m0.1479   \u001b[0m | \u001b[0m79.24    \u001b[0m | \u001b[0m2.579    \u001b[0m | \u001b[0m2.562    \u001b[0m | \u001b[0m0.1363   \u001b[0m | \u001b[0m94.61    \u001b[0m | \u001b[0m0.8777   \u001b[0m | \u001b[0m4.897    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.8973   \u001b[0m | \u001b[0m8.432    \u001b[0m | \u001b[0m739.0    \u001b[0m | \u001b[0m0.5944   \u001b[0m | \u001b[0m0.1035   \u001b[0m | \u001b[0m26.69    \u001b[0m | \u001b[0m2.159    \u001b[0m | \u001b[0m1.035    \u001b[0m | \u001b[0m0.5569   \u001b[0m | \u001b[0m66.93    \u001b[0m | \u001b[0m0.6784   \u001b[0m | \u001b[0m1.194    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.8295   \u001b[0m | \u001b[0m5.194    \u001b[0m | \u001b[0m364.8    \u001b[0m | \u001b[0m0.2515   \u001b[0m | \u001b[0m0.2908   \u001b[0m | \u001b[0m91.73    \u001b[0m | \u001b[0m1.246    \u001b[0m | \u001b[0m2.762    \u001b[0m | \u001b[0m0.9485   \u001b[0m | \u001b[0m51.39    \u001b[0m | \u001b[0m0.413    \u001b[0m | \u001b[0m4.04     \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 1s 4ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.6667   \u001b[0m | \u001b[0m0.7403   \u001b[0m | \u001b[0m919.9    \u001b[0m | \u001b[0m0.4112   \u001b[0m | \u001b[0m0.1232   \u001b[0m | \u001b[0m86.39    \u001b[0m | \u001b[0m2.893    \u001b[0m | \u001b[0m1.621    \u001b[0m | \u001b[0m0.752    \u001b[0m | \u001b[0m94.67    \u001b[0m | \u001b[0m0.3004   \u001b[0m | \u001b[0m5.003    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.5524   \u001b[0m | \u001b[0m4.506    \u001b[0m | \u001b[0m925.1    \u001b[0m | \u001b[0m0.4336   \u001b[0m | \u001b[0m0.05045  \u001b[0m | \u001b[0m83.14    \u001b[0m | \u001b[0m1.676    \u001b[0m | \u001b[0m1.772    \u001b[0m | \u001b[0m0.4987   \u001b[0m | \u001b[0m90.62    \u001b[0m | \u001b[0m0.4609   \u001b[0m | \u001b[0m1.909    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.5092   \u001b[0m | \u001b[0m4.932    \u001b[0m | \u001b[0m781.1    \u001b[0m | \u001b[0m0.3353   \u001b[0m | \u001b[0m0.2638   \u001b[0m | \u001b[0m50.25    \u001b[0m | \u001b[0m1.267    \u001b[0m | \u001b[0m1.248    \u001b[0m | \u001b[0m0.1769   \u001b[0m | \u001b[0m81.64    \u001b[0m | \u001b[0m0.2307   \u001b[0m | \u001b[0m2.394    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.191    \u001b[0m | \u001b[0m966.2    \u001b[0m | \u001b[0m0.04053  \u001b[0m | \u001b[0m0.2721   \u001b[0m | \u001b[0m78.01    \u001b[0m | \u001b[0m1.407    \u001b[0m | \u001b[0m2.463    \u001b[0m | \u001b[0m0.2568   \u001b[0m | \u001b[0m43.04    \u001b[0m | \u001b[0m0.2389   \u001b[0m | \u001b[0m4.9      \u001b[0m |\n",
      "=============================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "f1_scorer = make_scorer(fbeta_score, beta=1)\n",
    "\n",
    "def hypertuning_func2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    \n",
    "    act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU,'relu']\n",
    "    \n",
    "    neurons = round(neurons)\n",
    "    activation = act_list[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    \n",
    "    opt_list = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    \n",
    "    opt_dict= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    \n",
    "    optimizer = opt_dict[opt_list[round(optimizer)]]\n",
    "    \n",
    "    def temp_model2():\n",
    "        model2 = Sequential()\n",
    "        model2.add(Dense(neurons, input_dim=95, activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            model2.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            model2.add(Dense(neurons, activation=activation))\n",
    "        if dropout > 0.5:\n",
    "            model2.add(Dropout(dropout_rate, seed=123))\n",
    "        for i in range(layers2):\n",
    "            model2.add(Dense(neurons, activation=activation))\n",
    "        model2.add(Dense(1, activation='sigmoid'))\n",
    "        model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[get_f1])\n",
    "        return model2\n",
    "    \n",
    "    model2 = KerasClassifier(build_fn=temp_model2, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    strat_fold = Stratifiedstrat_fold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(model2, X_train_bal1, y_train_bal1, scoring=f1_scorer, cv=strat_fold).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "params_range2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "baye_opt2 = BayesianOptimization(hypertuning_func2, params_range2, random_state=111)\n",
    "baye_opt2.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "490b835f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softplus',\n",
       " 'batch_size': 892,\n",
       " 'dropout': 0.4820887768136817,\n",
       " 'dropout_rate': 0.020802538234812772,\n",
       " 'epochs': 49,\n",
       " 'layers1': 2,\n",
       " 'layers2': 2,\n",
       " 'learning_rate': 0.18768999923061017,\n",
       " 'neurons': 26,\n",
       " 'normalization': 0.9491296024792569,\n",
       " 'optimizer': <keras.optimizers.optimizer_v2.adamax.Adamax at 0x15ec4e610>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params2 = baye_opt2.max['params']\n",
    "learning_rate = tuned_params2['learning_rate']\n",
    "act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "\n",
    "tuned_params2['activation'] = act_list[round(tuned_params2['activation'])]\n",
    "tuned_params2['batch_size'] = round(tuned_params2['batch_size'])\n",
    "tuned_params2['epochs'] = round(tuned_params2['epochs'])\n",
    "tuned_params2['layers1'] = round(tuned_params2['layers1'])\n",
    "tuned_params2['layers2'] = round(tuned_params2['layers2'])\n",
    "tuned_params2['neurons'] = round(tuned_params2['neurons'])\n",
    "\n",
    "opt_list = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "opt_dict= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "\n",
    "tuned_params2['optimizer'] = opt_dict[opt_list[round(tuned_params2['optimizer'])]]\n",
    "tuned_params2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a47f4",
   "metadata": {},
   "source": [
    "<br id=\"7.2\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">7.2 Building the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97877365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/49\n",
      "10/10 [==============================] - 4s 57ms/step - loss: 0.6513 - get_f1: 0.7030 - val_loss: 0.6468 - val_get_f1: 0.1743\n",
      "Epoch 2/49\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.5676 - get_f1: 0.8169 - val_loss: 0.6057 - val_get_f1: 0.1826\n",
      "Epoch 3/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.4947 - get_f1: 0.8368 - val_loss: 0.5545 - val_get_f1: 0.1947\n",
      "Epoch 4/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.4251 - get_f1: 0.8555 - val_loss: 0.5291 - val_get_f1: 0.1927\n",
      "Epoch 5/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.3781 - get_f1: 0.8594 - val_loss: 0.5098 - val_get_f1: 0.1953\n",
      "Epoch 6/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.3453 - get_f1: 0.8664 - val_loss: 0.4918 - val_get_f1: 0.1983\n",
      "Epoch 7/49\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.3292 - get_f1: 0.8673 - val_loss: 0.4395 - val_get_f1: 0.2176\n",
      "Epoch 8/49\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.3193 - get_f1: 0.8693 - val_loss: 0.4060 - val_get_f1: 0.2289\n",
      "Epoch 9/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.3122 - get_f1: 0.8767 - val_loss: 0.3953 - val_get_f1: 0.2310\n",
      "Epoch 10/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.3059 - get_f1: 0.8781 - val_loss: 0.3561 - val_get_f1: 0.2456\n",
      "Epoch 11/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.3021 - get_f1: 0.8793 - val_loss: 0.3229 - val_get_f1: 0.2522\n",
      "Epoch 12/49\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.2966 - get_f1: 0.8790 - val_loss: 0.3076 - val_get_f1: 0.2549\n",
      "Epoch 13/49\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.2929 - get_f1: 0.8852 - val_loss: 0.3030 - val_get_f1: 0.2589\n",
      "Epoch 14/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2904 - get_f1: 0.8833 - val_loss: 0.2830 - val_get_f1: 0.2659\n",
      "Epoch 15/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2875 - get_f1: 0.8827 - val_loss: 0.2665 - val_get_f1: 0.2627\n",
      "Epoch 16/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2822 - get_f1: 0.8871 - val_loss: 0.2793 - val_get_f1: 0.2571\n",
      "Epoch 17/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2811 - get_f1: 0.8846 - val_loss: 0.2533 - val_get_f1: 0.2875\n",
      "Epoch 18/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2777 - get_f1: 0.8842 - val_loss: 0.2521 - val_get_f1: 0.2796\n",
      "Epoch 19/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2743 - get_f1: 0.8887 - val_loss: 0.2504 - val_get_f1: 0.2789\n",
      "Epoch 20/49\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.2711 - get_f1: 0.8895 - val_loss: 0.2501 - val_get_f1: 0.2341\n",
      "Epoch 21/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2684 - get_f1: 0.8894 - val_loss: 0.2450 - val_get_f1: 0.2402\n",
      "Epoch 22/49\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.2673 - get_f1: 0.8880 - val_loss: 0.2367 - val_get_f1: 0.2483\n",
      "Epoch 23/49\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.2634 - get_f1: 0.8912 - val_loss: 0.2363 - val_get_f1: 0.2476\n",
      "Epoch 24/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2608 - get_f1: 0.8921 - val_loss: 0.2442 - val_get_f1: 0.2396\n",
      "Epoch 25/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2591 - get_f1: 0.8931 - val_loss: 0.2271 - val_get_f1: 0.2523\n",
      "Epoch 26/49\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.2582 - get_f1: 0.8934 - val_loss: 0.2313 - val_get_f1: 0.2502\n",
      "Epoch 27/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2528 - get_f1: 0.8989 - val_loss: 0.2353 - val_get_f1: 0.2430\n",
      "Epoch 28/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2520 - get_f1: 0.8976 - val_loss: 0.2285 - val_get_f1: 0.2493\n",
      "Epoch 29/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2492 - get_f1: 0.8976 - val_loss: 0.2263 - val_get_f1: 0.2491\n",
      "Epoch 30/49\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.2488 - get_f1: 0.8992 - val_loss: 0.2377 - val_get_f1: 0.2394\n",
      "Epoch 31/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2459 - get_f1: 0.9013 - val_loss: 0.2302 - val_get_f1: 0.2454\n",
      "Epoch 32/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2434 - get_f1: 0.8984 - val_loss: 0.2290 - val_get_f1: 0.2441\n",
      "Epoch 33/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2417 - get_f1: 0.9007 - val_loss: 0.2302 - val_get_f1: 0.2441\n",
      "Epoch 34/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2394 - get_f1: 0.9048 - val_loss: 0.2223 - val_get_f1: 0.2499\n",
      "Epoch 35/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2413 - get_f1: 0.9030 - val_loss: 0.2313 - val_get_f1: 0.2439\n",
      "Epoch 36/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2364 - get_f1: 0.9068 - val_loss: 0.2212 - val_get_f1: 0.2534\n",
      "Epoch 37/49\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2341 - get_f1: 0.9068 - val_loss: 0.2258 - val_get_f1: 0.2492\n",
      "Epoch 38/49\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.2323 - get_f1: 0.9081 - val_loss: 0.2382 - val_get_f1: 0.2424\n",
      "Epoch 39/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2309 - get_f1: 0.9122 - val_loss: 0.2247 - val_get_f1: 0.2501\n",
      "Epoch 40/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2303 - get_f1: 0.9109 - val_loss: 0.2357 - val_get_f1: 0.2466\n",
      "Epoch 41/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2265 - get_f1: 0.9155 - val_loss: 0.2263 - val_get_f1: 0.2478\n",
      "Epoch 42/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2266 - get_f1: 0.9130 - val_loss: 0.2343 - val_get_f1: 0.2489\n",
      "Epoch 43/49\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2257 - get_f1: 0.9131 - val_loss: 0.2272 - val_get_f1: 0.2547\n",
      "Epoch 44/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2231 - get_f1: 0.9149 - val_loss: 0.2226 - val_get_f1: 0.2668\n",
      "Epoch 45/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2209 - get_f1: 0.9138 - val_loss: 0.2284 - val_get_f1: 0.2617\n",
      "Epoch 46/49\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.2191 - get_f1: 0.9142 - val_loss: 0.2381 - val_get_f1: 0.2465\n",
      "Epoch 47/49\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.2155 - get_f1: 0.9181 - val_loss: 0.2188 - val_get_f1: 0.2733\n",
      "Epoch 48/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2138 - get_f1: 0.9200 - val_loss: 0.2254 - val_get_f1: 0.2686\n",
      "Epoch 49/49\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2136 - get_f1: 0.9196 - val_loss: 0.2386 - val_get_f1: 0.2559\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=26, input_dim=95, activation='softplus'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(units=26, activation='softplus'))\n",
    "model1.add(Dense(units=26, activation='softplus'))\n",
    "model1.add(Dense(units=26, activation='softplus'))\n",
    "model1.add(Dense(units=26, activation='softplus'))\n",
    "model1.add(Dropout(0.020802538234812772))\n",
    "model1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy', optimizer=Adamax(lr = 0.18768999923061017), metrics=[get_f1])\n",
    "\n",
    "history1 = model1.fit(X_train_bal1, y_train_bal1,batch_size=892, epochs=49, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae853ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.95      1824\n",
      "           1       0.19      0.74      0.31        57\n",
      "\n",
      "    accuracy                           0.90      1881\n",
      "   macro avg       0.59      0.82      0.63      1881\n",
      "weighted avg       0.97      0.90      0.93      1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model,x):\n",
    "    pred  = model.predict(x)\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    return pred\n",
    "\n",
    "def predict_graph(y_true,y_pred,title):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(cm,annot=True,fmt='g',cmap=\"YlGnBu\",\n",
    "                xticklabels=['No Bankruptcy','Bankruptcy'],yticklabels=['No Bankruptcy','Bankruptcy'])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true,y_pred))\n",
    "\n",
    "\n",
    "y_test_pred = predict(model1,X_test)\n",
    "predict_graph(y_test,y_test_pred,'Test Data Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79703c5",
   "metadata": {},
   "source": [
    "<br id=\"8\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 22px; text-decoration: none; font-weight:bold;\"> 8. Model 3: 5-layer-ANN-PCA-BSM </a>\n",
    "<hr style=\"height:1px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec2f59",
   "metadata": {},
   "source": [
    "<br id=\"8.1\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">8.1 PCA</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9189b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA (90%)\n",
    "pca = PCA(n_components=0.90)  # WE DO THIS TO PREVENT OVERFITTING\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7e562",
   "metadata": {},
   "source": [
    "<br id=\"8.2\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">8.2 BSM Oversampling</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f416a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform oversampling on the training data using Borderline SMOTE\n",
    "\n",
    "sm2 = BorderlineSMOTE(random_state=42)\n",
    "X_train_bal2, y_train_bal2 = sm2.fit_resample(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24dfd2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4228\n",
      "1    4228\n",
      "Name: Bankrupt?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_counts = y_train_bal2.value_counts()\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1d25360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8456, 41), (1881, 41), (8456,), (1881,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal2.shape,X_test_pca.shape,y_train_bal2.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628f7f6",
   "metadata": {},
   "source": [
    "<br id=\"8.3\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">8.3 Tuning the hyperparameters</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9adb3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8415   \u001b[0m | \u001b[0m5.51     \u001b[0m | \u001b[0m335.3    \u001b[0m | \u001b[0m54.88    \u001b[0m | \u001b[0m0.7716   \u001b[0m | \u001b[0m36.58    \u001b[0m | \u001b[0m1.044    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.9615   \u001b[0m | \u001b[95m0.2023   \u001b[0m | \u001b[95m536.2    \u001b[0m | \u001b[95m39.09    \u001b[0m | \u001b[95m0.3443   \u001b[0m | \u001b[95m99.16    \u001b[0m | \u001b[95m1.664    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.3907   \u001b[0m | \u001b[0m0.7307   \u001b[0m | \u001b[0m735.7    \u001b[0m | \u001b[0m69.7     \u001b[0m | \u001b[0m0.2815   \u001b[0m | \u001b[0m51.96    \u001b[0m | \u001b[0m0.8286   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.1333   \u001b[0m | \u001b[0m0.6656   \u001b[0m | \u001b[0m920.6    \u001b[0m | \u001b[0m83.52    \u001b[0m | \u001b[0m0.8422   \u001b[0m | \u001b[0m83.37    \u001b[0m | \u001b[0m6.937    \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.9834   \u001b[0m | \u001b[95m5.195    \u001b[0m | \u001b[95m851.0    \u001b[0m | \u001b[95m53.71    \u001b[0m | \u001b[95m0.03717  \u001b[0m | \u001b[95m50.87    \u001b[0m | \u001b[95m0.7373   \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.355    \u001b[0m | \u001b[0m758.2    \u001b[0m | \u001b[0m65.22    \u001b[0m | \u001b[0m0.2815   \u001b[0m | \u001b[0m99.86    \u001b[0m | \u001b[0m0.9663   \u001b[0m |\n",
      "53/53 [==============================] - 0s 3ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8876   \u001b[0m | \u001b[0m5.539    \u001b[0m | \u001b[0m588.0    \u001b[0m | \u001b[0m52.4     \u001b[0m | \u001b[0m0.7306   \u001b[0m | \u001b[0m39.05    \u001b[0m | \u001b[0m2.804    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m2.871    \u001b[0m | \u001b[0m957.8    \u001b[0m | \u001b[0m93.5     \u001b[0m | \u001b[0m0.8157   \u001b[0m | \u001b[0m13.07    \u001b[0m | \u001b[0m6.604    \u001b[0m |\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8047   \u001b[0m | \u001b[0m8.554    \u001b[0m | \u001b[0m845.3    \u001b[0m | \u001b[0m58.5     \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m47.53    \u001b[0m | \u001b[0m2.232    \u001b[0m |\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.964    \u001b[0m | \u001b[0m0.148    \u001b[0m | \u001b[0m230.5    \u001b[0m | \u001b[0m24.25    \u001b[0m | \u001b[0m0.1367   \u001b[0m | \u001b[0m13.0     \u001b[0m | \u001b[0m1.585    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9678   \u001b[0m | \u001b[0m4.895    \u001b[0m | \u001b[0m342.9    \u001b[0m | \u001b[0m34.35    \u001b[0m | \u001b[0m0.1581   \u001b[0m | \u001b[0m71.47    \u001b[0m | \u001b[0m3.283    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.914    \u001b[0m | \u001b[0m735.1    \u001b[0m | \u001b[0m55.3     \u001b[0m | \u001b[0m0.5993   \u001b[0m | \u001b[0m51.55    \u001b[0m | \u001b[0m6.743    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8415   \u001b[0m | \u001b[0m1.33     \u001b[0m | \u001b[0m925.5    \u001b[0m | \u001b[0m59.83    \u001b[0m | \u001b[0m0.5966   \u001b[0m | \u001b[0m71.62    \u001b[0m | \u001b[0m1.242    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9276   \u001b[0m | \u001b[0m7.782    \u001b[0m | \u001b[0m585.7    \u001b[0m | \u001b[0m25.55    \u001b[0m | \u001b[0m0.3711   \u001b[0m | \u001b[0m42.54    \u001b[0m | \u001b[0m3.304    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.4415   \u001b[0m | \u001b[0m1.615    \u001b[0m | \u001b[0m340.2    \u001b[0m | \u001b[0m95.93    \u001b[0m | \u001b[0m0.6591   \u001b[0m | \u001b[0m22.15    \u001b[0m | \u001b[0m6.495    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9616   \u001b[0m | \u001b[0m7.576    \u001b[0m | \u001b[0m242.2    \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m0.8738   \u001b[0m | \u001b[0m70.65    \u001b[0m | \u001b[0m2.081    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 4ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m6.61     \u001b[0m | \u001b[0m694.7    \u001b[0m | \u001b[0m36.84    \u001b[0m | \u001b[0m0.804    \u001b[0m | \u001b[0m15.32    \u001b[0m | \u001b[0m2.158    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5239   \u001b[0m | \u001b[0m1.866    \u001b[0m | \u001b[0m977.8    \u001b[0m | \u001b[0m92.75    \u001b[0m | \u001b[0m0.6797   \u001b[0m | \u001b[0m20.37    \u001b[0m | \u001b[0m6.706    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.4607   \u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m703.8    \u001b[0m | \u001b[0m92.23    \u001b[0m | \u001b[0m0.3464   \u001b[0m | \u001b[0m68.75    \u001b[0m | \u001b[0m6.476    \u001b[0m |\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9509   \u001b[0m | \u001b[0m3.366    \u001b[0m | \u001b[0m817.1    \u001b[0m | \u001b[0m91.69    \u001b[0m | \u001b[0m0.624    \u001b[0m | \u001b[0m23.6     \u001b[0m | \u001b[0m2.624    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 1s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9708   \u001b[0m | \u001b[0m5.723    \u001b[0m | \u001b[0m567.3    \u001b[0m | \u001b[0m62.58    \u001b[0m | \u001b[0m0.3588   \u001b[0m | \u001b[0m69.39    \u001b[0m | \u001b[0m3.336    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9434   \u001b[0m | \u001b[0m4.091    \u001b[0m | \u001b[0m299.8    \u001b[0m | \u001b[0m53.0     \u001b[0m | \u001b[0m0.2804   \u001b[0m | \u001b[0m41.21    \u001b[0m | \u001b[0m6.821    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.3156   \u001b[0m | \u001b[0m1.94     \u001b[0m | \u001b[0m746.3    \u001b[0m | \u001b[0m22.54    \u001b[0m | \u001b[0m0.837    \u001b[0m | \u001b[0m73.15    \u001b[0m | \u001b[0m6.762    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.982    \u001b[0m | \u001b[0m5.326    \u001b[0m | \u001b[0m373.9    \u001b[0m | \u001b[0m77.54    \u001b[0m | \u001b[0m0.04056  \u001b[0m | \u001b[0m47.68    \u001b[0m | \u001b[0m1.969    \u001b[0m |\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.786    \u001b[0m | \u001b[0m0.9562   \u001b[0m | \u001b[0m541.1    \u001b[0m | \u001b[0m87.25    \u001b[0m | \u001b[0m0.1193   \u001b[0m | \u001b[0m98.8     \u001b[0m | \u001b[0m1.633    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "| \u001b[95m26       \u001b[0m | \u001b[95m0.9841   \u001b[0m | \u001b[95m0.0      \u001b[0m | \u001b[95m885.2    \u001b[0m | \u001b[95m36.11    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m52.9     \u001b[0m | \u001b[95m0.0      \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.9688   \u001b[0m | \u001b[0m0.785    \u001b[0m | \u001b[0m925.7    \u001b[0m | \u001b[0m63.19    \u001b[0m | \u001b[0m0.2954   \u001b[0m | \u001b[0m14.66    \u001b[0m | \u001b[0m5.517    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9761   \u001b[0m | \u001b[0m5.657    \u001b[0m | \u001b[0m935.6    \u001b[0m | \u001b[0m27.78    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m49.16    \u001b[0m | \u001b[0m0.0      \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adamax.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 2ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "53/53 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.9814   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m852.1    \u001b[0m | \u001b[0m23.07    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m75.66    \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "f1_scorer = make_scorer(fbeta_score, beta=1)\n",
    "\n",
    "def hypertuning_func3(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    \n",
    "   \n",
    "    act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU,'relu']\n",
    "    \n",
    "    neurons = round(neurons)\n",
    "    activation = act_list[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    \n",
    "    opt_list = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    \n",
    "    opt_dict= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    \n",
    "    \n",
    "    def temp_model3():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        model3 = Sequential()\n",
    "        model3.add(Dense(neurons, input_dim=41, activation=activation))\n",
    "        model3.add(Dense(neurons, activation=activation))\n",
    "        model3.add(Dense(1, activation='sigmoid')) \n",
    "        model3.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "        return model3\n",
    "    \n",
    "    model3 = KerasClassifier(build_fn=temp_model3, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0) #creates a Stratifiedstrat_fold object converting data into folds for cross-validation\n",
    "    \n",
    "    strat_fold = Stratifiedstrat_fold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    score = cross_val_score(model3, X_train_bal2, y_train_bal2, scoring=f1_scorer, cv=strat_fold).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Set paramaters\n",
    "params_range3 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "baye_opt3 = BayesianOptimization(hypertuning_func3, params_range3, random_state=111)\n",
    "baye_opt3.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4490d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 885.214632751467,\n",
       " 'epochs': 36.11298734601822,\n",
       " 'learning_rate': 0.01,\n",
       " 'neurons': 52.89744733549892,\n",
       " 'optimizer': 0.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params3 = baye_opt3.max['params']\n",
    "act_list = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU,'relu']\n",
    "\n",
    "tuned_params3['activation'] = act_list[round(tuned_params3['activation'])]\n",
    "tuned_params3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2539d",
   "metadata": {},
   "source": [
    "<br id=\"8.4\">\n",
    "<a href=\"#index\" style=\"color:#033a91; font-size: 16px; text-decoration: none; font-weight:bold;\">8.4 Building the model</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a0fb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 0.3616 - get_f1: 0.8774 - val_loss: 0.2125 - val_get_f1: 0.3452\n",
      "Epoch 2/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.1885 - get_f1: 0.9359 - val_loss: 0.2260 - val_get_f1: 0.3817\n",
      "Epoch 3/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1568 - get_f1: 0.9499 - val_loss: 0.2072 - val_get_f1: 0.3501\n",
      "Epoch 4/36\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.1325 - get_f1: 0.9577 - val_loss: 0.1821 - val_get_f1: 0.3290\n",
      "Epoch 5/36\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.1100 - get_f1: 0.9656 - val_loss: 0.1784 - val_get_f1: 0.2431\n",
      "Epoch 6/36\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0971 - get_f1: 0.9724 - val_loss: 0.1803 - val_get_f1: 0.2542\n",
      "Epoch 7/36\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0857 - get_f1: 0.9760 - val_loss: 0.1972 - val_get_f1: 0.2564\n",
      "Epoch 8/36\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0755 - get_f1: 0.9777 - val_loss: 0.2132 - val_get_f1: 0.2502\n",
      "Epoch 9/36\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0677 - get_f1: 0.9805 - val_loss: 0.2047 - val_get_f1: 0.2708\n",
      "Epoch 10/36\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.0596 - get_f1: 0.9830 - val_loss: 0.2041 - val_get_f1: 0.2697\n",
      "Epoch 11/36\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.0596 - get_f1: 0.9840 - val_loss: 0.2592 - val_get_f1: 0.2533\n",
      "Epoch 12/36\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0552 - get_f1: 0.9834 - val_loss: 0.2178 - val_get_f1: 0.2742\n",
      "Epoch 13/36\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.0508 - get_f1: 0.9846 - val_loss: 0.2130 - val_get_f1: 0.2533\n",
      "Epoch 14/36\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0431 - get_f1: 0.9878 - val_loss: 0.2191 - val_get_f1: 0.2424\n",
      "Epoch 15/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0361 - get_f1: 0.9894 - val_loss: 0.2307 - val_get_f1: 0.2440\n",
      "Epoch 16/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0321 - get_f1: 0.9908 - val_loss: 0.2448 - val_get_f1: 0.2376\n",
      "Epoch 17/36\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0297 - get_f1: 0.9922 - val_loss: 0.2565 - val_get_f1: 0.2283\n",
      "Epoch 18/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0271 - get_f1: 0.9925 - val_loss: 0.2689 - val_get_f1: 0.2622\n",
      "Epoch 19/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0240 - get_f1: 0.9935 - val_loss: 0.2643 - val_get_f1: 0.2409\n",
      "Epoch 20/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0217 - get_f1: 0.9946 - val_loss: 0.2868 - val_get_f1: 0.2451\n",
      "Epoch 21/36\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0240 - get_f1: 0.9931 - val_loss: 0.2764 - val_get_f1: 0.2503\n",
      "Epoch 22/36\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0229 - get_f1: 0.9943 - val_loss: 0.2743 - val_get_f1: 0.2160\n",
      "Epoch 23/36\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0182 - get_f1: 0.9947 - val_loss: 0.2883 - val_get_f1: 0.2409\n",
      "Epoch 24/36\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0136 - get_f1: 0.9960 - val_loss: 0.2893 - val_get_f1: 0.2143\n",
      "Epoch 25/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0120 - get_f1: 0.9970 - val_loss: 0.3032 - val_get_f1: 0.2358\n",
      "Epoch 26/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0096 - get_f1: 0.9975 - val_loss: 0.3121 - val_get_f1: 0.2373\n",
      "Epoch 27/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0084 - get_f1: 0.9980 - val_loss: 0.3270 - val_get_f1: 0.2317\n",
      "Epoch 28/36\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0091 - get_f1: 0.9978 - val_loss: 0.3441 - val_get_f1: 0.2449\n",
      "Epoch 29/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0119 - get_f1: 0.9963 - val_loss: 0.3185 - val_get_f1: 0.2252\n",
      "Epoch 30/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0161 - get_f1: 0.9970 - val_loss: 0.3377 - val_get_f1: 0.2132\n",
      "Epoch 31/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0160 - get_f1: 0.9951 - val_loss: 0.3496 - val_get_f1: 0.2122\n",
      "Epoch 32/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0235 - get_f1: 0.9943 - val_loss: 0.3934 - val_get_f1: 0.2070\n",
      "Epoch 33/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0134 - get_f1: 0.9961 - val_loss: 0.3437 - val_get_f1: 0.2293\n",
      "Epoch 34/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0100 - get_f1: 0.9970 - val_loss: 0.3839 - val_get_f1: 0.2139\n",
      "Epoch 35/36\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0083 - get_f1: 0.9984 - val_loss: 0.3016 - val_get_f1: 0.2242\n",
      "Epoch 36/36\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0115 - get_f1: 0.9979 - val_loss: 0.3183 - val_get_f1: 0.2246\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(round(tuned_params3['neurons']), input_dim=41, activation=tuned_params3['activation']))\n",
    "nn.add(Dense(round(tuned_params3['neurons']), activation=tuned_params3['activation']))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "nn.compile(loss='binary_crossentropy', optimizer=Adam(lr = tuned_params3['learning_rate']), metrics=[get_f1])\n",
    "\n",
    "history = nn.fit(X_train_bal2, y_train_bal2,batch_size=round(tuned_params3['batch_size']), \n",
    "                        epochs=round(tuned_params3['epochs']), validation_data=(X_test_pca,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3759ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1824\n",
      "           1       0.35      0.30      0.32        57\n",
      "\n",
      "    accuracy                           0.96      1881\n",
      "   macro avg       0.67      0.64      0.65      1881\n",
      "weighted avg       0.96      0.96      0.96      1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model,x):\n",
    "    pred  = model.predict(x)\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    return pred\n",
    "\n",
    "def predict_graph(y_true,y_pred,title):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(cm,annot=True,fmt='g',cmap=\"YlGnBu\",\n",
    "                xticklabels=['No Bankruptcy','Bankruptcy'],yticklabels=['No Bankruptcy','Bankruptcy'])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true,y_pred))\n",
    "\n",
    "\n",
    "y_test_pred = predict(nn,X_test_pca)\n",
    "predict_graph(y_test,y_test_pred,'Test Data Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4f3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
